<!DOCTYPE html>
<html>
  <head>
    <title>Consuming Storage From external ceph cluster in Rook â€“ Madhu Rajanna â€“ Senior Software Engineer at RedHat</title>

        <meta charset="utf-8" />
    <meta content='text/html; charset=utf-8' http-equiv='Content-Type'>
    <meta http-equiv='X-UA-Compatible' content='IE=edge'>
    <meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1.0'>

    
    <meta name="description" content="This doc assumes that you already set up the Rook with an external ceph cluster.

" />
    <meta property="og:description" content="This doc assumes that you already set up the Rook with an external ceph cluster.

" />
    
    <meta name="author" content="Madhu Rajanna" />

    
    <meta property="og:title" content="Consuming Storage From external ceph cluster in Rook" />
    <meta property="twitter:title" content="Consuming Storage From external ceph cluster in Rook" />
    
    <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Consuming Storage From external ceph cluster in Rook | Madhu Rajanna</title>
<meta name="generator" content="Jekyll v3.9.2" />
<meta property="og:title" content="Consuming Storage From external ceph cluster in Rook" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="This doc assumes that you already set up the Rook with an external ceph cluster." />
<meta property="og:description" content="This doc assumes that you already set up the Rook with an external ceph cluster." />
<link rel="canonical" href="https://mrajanna.com/Consuming-storage-from-exteranal-ceph-in-rook/" />
<meta property="og:url" content="https://mrajanna.com/Consuming-storage-from-exteranal-ceph-in-rook/" />
<meta property="og:site_name" content="Madhu Rajanna" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-06-10T00:00:00+05:30" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Consuming Storage From external ceph cluster in Rook" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2022-06-10T00:00:00+05:30","datePublished":"2022-06-10T00:00:00+05:30","description":"This doc assumes that you already set up the Rook with an external ceph cluster.","headline":"Consuming Storage From external ceph cluster in Rook","mainEntityOfPage":{"@type":"WebPage","@id":"https://mrajanna.com/Consuming-storage-from-exteranal-ceph-in-rook/"},"url":"https://mrajanna.com/Consuming-storage-from-exteranal-ceph-in-rook/"}</script>
<!-- End Jekyll SEO tag -->
<meta name="google-site-verification" content="qrFYIlFxt1yD_oRmFPV2ZpEOYn46EXVxnRpg2HhMaUU" />
  <link rel="stylesheet" href="/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://mrajanna.com/feed.xml" title="Madhu Rajanna" />
	<!-- Google Analytics -->
	<script>
		(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
		(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
		m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
		})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

		ga('create', 'qrFYIlFxt1yD_oRmFPV2ZpEOYn46EXVxnRpg2HhMaUU', 'auto');
		ga('send', 'pageview', {
		  'page': '/Consuming-storage-from-exteranal-ceph-in-rook/',
		  'title': 'Consuming Storage From external ceph cluster in Rook'
		});
	</script>
	<!-- End Google Analytics -->

</head>


    <!--[if lt IE 9]>
      <script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->

    <link rel="stylesheet" type="text/css" href="/style.css" />
    <link rel="alternate" type="application/rss+xml" title="Madhu Rajanna - Senior Software Engineer at RedHat" href="/feed.xml" />

    <!-- Created with Jekyll Now - http://github.com/barryclark/jekyll-now -->
  </head>

  <body>
    <div class="wrapper-masthead">
      <div class="container">
        <header class="masthead clearfix">
          <a href="/" class="site-avatar"><img src="https://raw.githubusercontent.com/Madhu-1/Madhu-1.github.io/master/images/Madhu.jpeg" /></a>
          <nav>
            <a href="/">Blog</a>
            <a href="/about">about</a>
            <a href="/resume">Resume</a>
          </nav>
          <br />
          <br />
          <br />
          <br />
          <br />
          <br />
          <br />
          <div class="site-info">
            <h1 class="site-name"><strong><a href="/">Madhu Rajanna</a></strong></h1>
          </div>
        </header>
      </div>
    </div>

    <div id="main" role="main" class="container">
      <article class="post">
  <h1>Consuming Storage From external ceph cluster in Rook</h1>

  <div class="entry">
    <p>This doc assumes that you already set up the Rook with an external ceph cluster.</p>

<p>Please refer to <a href="../setup-external-ceph-cluster-with-rook">doc</a> on how to setup
external ceph cluster with Rook.</p>

<p>This blog will help you to Understand how to consume storage from imported
external ceph cluster in Rook and how to create and consume Below listed
resources in ceph cluster.</p>

<ul>
  <li>CephBlockPool</li>
  <li>CephFileSystem</li>
  <li>SubvolumeGroup</li>
  <li>RadosNamespace</li>
</ul>

<h2 id="create-resources-specified-above-in-the-ceph-cluster">Create resources specified above in the ceph cluster</h2>

<p><strong>Note</strong> You can skip this step and create the ceph resources with Rook CRs if
you are using the ceph cluster deployed with Rook.</p>

<h3 id="create-rbd-pool">Create RBD Pool</h3>

<pre><code class="language-bash=">ceph osd pool create replicapool 128
rbd pool init
ceph osd pool set replicapool size 1 --yes-i-really-mean-it
</code></pre>

<h3 id="create-cephfs-filesystem">Create CephFS Filesystem</h3>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="go">ceph osd pool create myfs-metadata 128
ceph osd pool create myfs-data 124
ceph fs new myfs myfs-metadata myfs-data
</span></code></pre></div></div>

<h3 id="create-subvolumegroup">Create SubVolumeGroup</h3>

<p>Letâ€™s create a SubVolumeGroup in the Filesystem <code class="language-plaintext highlighter-rouge">myfs</code> created above</p>

<pre><code class="language-bash=">ceph fs subvolumegroup create myfs volgroup
</code></pre>

<h3 id="create-radosnamespace">Create RadosNamespace</h3>

<p>Letâ€™s create a RadosNamespace in the BlockPool <code class="language-plaintext highlighter-rouge">replicapool</code> created above</p>

<pre><code class="language-bash=">rbd namespace create replicapool/testnamespace
</code></pre>

<p>checkout Rook 1.9.5 Repository</p>

<pre><code class="language-bash=">cd rook/deploy/examples
</code></pre>

<h2 id="consume-above-created-ceph-resources-in-kubernetes-cluster">Consume above created ceph resources in kubernetes cluster</h2>

<h3 id="cephblockpool">CephBlockPool</h3>

<p>To create RBD PVC, we need to first have the RBD pool created in the ceph
cluster and ceph users to access the pool and a StorageClass to point to the
Pool where to create the PVC.</p>

<p><em>Note</em> Above create a pool with the name <code class="language-plaintext highlighter-rouge">replicapool</code> with  PG Num <code class="language-plaintext highlighter-rouge">128</code> and
setting replica size to <code class="language-plaintext highlighter-rouge">1</code> As I have only one OSD. If you have a minimum <code class="language-plaintext highlighter-rouge">3</code>
OSD please set it to <code class="language-plaintext highlighter-rouge">3</code>.</p>

<p>As this ceph cluster is deployed in the <code class="language-plaintext highlighter-rouge">rook-ceph-external</code> namespace, we need
to update a few details in RBD storageclass</p>

<pre><code class="language-bash=">$ cd csi/rbd
$ cat storageclass-test.yaml
allowVolumeExpansion: true
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: rook-ceph-block
parameters:
  clusterID: rook-ceph-external  # Namespace where ceph cluster is deployed
  csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
  csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph-external  # Namespace where ceph cluster is deployed
  csi.storage.k8s.io/fstype: ext4
  csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
  csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph-external  # Namespace where ceph cluster is deployed
  csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
  csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph-external  # Namespace where ceph cluster is deployed
  imageFeatures: layering
  imageFormat: "2"
  pool: replicapool
provisioner: rook-ceph.rbd.csi.ceph.com
reclaimPolicy: Delete
volumeBindingMode: Immediate
</code></pre>

<p>change the above parameters in the rbd storageclass-test.yaml and keep all
other configurations as it is (Dont forget to remove pool.yaml from
storageclass-test.yaml).</p>

<p>Letâ€™s create RBD storageclass, PVC, Pod and verify it on the ceph cluster.</p>

<pre><code class="language-bash=">[ðŸŽ©ï¸Ž]mrajanna@fedora rbd $]kubectl create -f storageclass-test.yaml
storageclass.storage.k8s.io/rook-ceph-block created
[ðŸŽ©ï¸Ž]mrajanna@fedora rbd $]kubectl create -f pvc.yaml
persistentvolumeclaim/rbd-pvc created
[ðŸŽ©ï¸Ž]mrajanna@fedora rbd $]kubectl get pvc
NAME      STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS      AGE
rbd-pvc   Bound    pvc-a1a802fd-a577-45e4-b52d-687b8333cbef   1Gi        RWO            rook-ceph-block   4s
[ðŸŽ©ï¸Ž]mrajanna@fedora rbd $]kubectl create -f pod.yaml
pod/csirbd-demo-pod created
[ðŸŽ©ï¸Ž]mrajanna@fedora rbd $]kubectl get po
NAME              READY   STATUS    RESTARTS   AGE
csirbd-demo-pod   1/1     Running   0          43s
</code></pre>

<p>Letâ€™s verify we have the rbd image in the ceph cluster.</p>

<pre><code class="language-bash=">sh-4.4$ rbd ls replicapool
csi-vol-9475df3e-e895-11ec-bbf0-9a1ab8207c3a
</code></pre>

<h3 id="cephfilesystem">CephFilesystem</h3>

<p>To create CephFS PVC, we need to first have the CephFS Filesystem created in
the ceph cluster and ceph users to access the Filesystem and a StorageClass to
point to the Filesystem where to create the PVC.</p>

<p><em>Note</em> Above a Filesystem with the name <code class="language-plaintext highlighter-rouge">myfs</code>, letâ€™s use the same for testing</p>

<p>As this ceph cluster is deployed in the <code class="language-plaintext highlighter-rouge">rook-ceph-external</code> namespace, we need
to update a few details in CephFS storageclass</p>

<pre><code class="language-bash=">$ cd csi/cephfs
$ cat storageclass.yaml
allowVolumeExpansion: true
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: rook-cephfs
parameters:
  clusterID: rook-ceph-external # Namespace where ceph cluster is deployed
  csi.storage.k8s.io/controller-expand-secret-name: rook-csi-cephfs-provisioner
  csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph-external # Namespace where ceph cluster is deployed
  csi.storage.k8s.io/node-stage-secret-name: rook-csi-cephfs-node
  csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph-external # Namespace where ceph cluster is deployed
  csi.storage.k8s.io/provisioner-secret-name: rook-csi-cephfs-provisioner
  csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph-external # Namespace where ceph cluster is deployed
  fsName: myfs
  pool: myfs-data
provisioner: rook-ceph.cephfs.csi.ceph.com
reclaimPolicy: Delete
volumeBindingMode: Immediate
</code></pre>

<p>change the above parameters in the cephfs storageclass.yaml and keep all other
configurations as it is.</p>

<p>Letâ€™s create CephFS storageclass, PVC, Pod and verify it on the ceph cluster.</p>

<pre><code class="language-bash=">[ðŸŽ©ï¸Ž]mrajanna@fedora cephfs $]kubectl create  -f cephfs/storageclass.yaml
storageclass.storage.k8s.io/rook-cephfs created
[ðŸŽ©ï¸Ž]mrajanna@fedora cephfs $]kubectl create -f pvc.yaml
persistentvolumeclaim/cephfs-pvc created
[ðŸŽ©ï¸Ž]mrajanna@fedora cephfs $]kubectl get pvc
NAME         STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS      AGE
cephfs-pvc   Bound    pvc-52186bd0-92dd-433c-af00-e0a60baf71a1   1Gi        RWO            rook-cephfs       2s
[ðŸŽ©ï¸Ž]mrajanna@fedora cephfs $]kubectl create -f pod.yaml
pod/csicephfs-demo-pod created
[ðŸŽ©ï¸Ž]mrajanna@fedora cephfs $]kubectl get po
NAME                 READY   STATUS    RESTARTS   AGE
csicephfs-demo-pod   1/1     Running   0          24s
</code></pre>

<p>Letâ€™s verify we have cephfs Subvolume created in the ceph cluster.</p>

<pre><code class="language-bash=">sh-4.4$ ceph fs subvolume ls myfs --group_name=csi
[
    {
        "name": "csi-vol-9bcadef0-e896-11ec-93d2-8a68b48559cc"
    }
]
</code></pre>

<p><strong>Note</strong> <code class="language-plaintext highlighter-rouge">csi</code> is the default subvolumegroup cephcsi creates.</p>

<h3 id="subvolumegroup">SubvolumeGroup</h3>

<p>To create CephFS PVC in a subvolumeGroup, we need to first have subvolumegroup
created in the CephFS Filesystem created in the ceph cluster and ceph users to
access the subvolumegroup and a StorageClass to point to the clusterID of
subvolumegroup where to create the PVC.</p>

<p><em>Note</em> Above we have subvolumegroup with name <code class="language-plaintext highlighter-rouge">volgroup</code> in <code class="language-plaintext highlighter-rouge">myfs</code> Filesystem,
Letâ€™s use the same for testing</p>

<h4 id="create-subvolumegroup-cr">Create SubVolumeGroup CR</h4>

<pre><code class="language-bash=">[ðŸŽ©ï¸Ž]mrajanna@fedora examples $]cat &lt;&lt;EOFcat &lt;&lt;EOF | kubectl apply -f -
---
apiVersion: ceph.rook.io/v1
kind: CephFilesystemSubVolumeGroup
metadata:
  name: volgroup
  namespace: rook-ceph-external
spec:
  filesystemName: myfs
EOF
cephfilesystemsubvolumegroup.ceph.rook.io/volgroup created
</code></pre>

<p><strong>Note</strong> We are creating SubVolumeGroup CR to generate the SubVolumeGroup Name
and ClusterID mapping, it wonâ€™t create SubVolumeGroup in the Ceph cluster.</p>

<h4 id="check-subvolumegroup-details">Check SubVolumeGroup details</h4>

<pre><code class="language-bash=">[ðŸŽ©ï¸Ž]mrajanna@fedora examples $]kubectl get cephfilesystemsubvolumegroup.ceph.rook.io/volgroup -nrook-ceph-external
NAME       PHASE
volgroup   Ready
[ðŸŽ©ï¸Ž]mrajanna@fedora examples $]kubectl get cephfilesystemsubvolumegroup.ceph.rook.io/volgroup -nrook-ceph-external -o jsonpath='{.status.info}'
{"clusterID":"728ed87ac1a8809431b4549f4a4897aa"}
</code></pre>

<p>The above clusterID <code class="language-plaintext highlighter-rouge">728ed87ac1a8809431b4549f4a4897aa</code> is the one we need to
use it in subvolumegroup storageclass.</p>

<p>The reason we have a new clusterID for subvolumegroup is we cannot specify
subvolumegroup name in the storageclass, This clusterID has the mapping to the
subvolumegroup</p>

<h4 id="check-mapping-between-clusterid-and-subvolumegroup">Check mapping between clusterID and SubVolumeGroup</h4>

<pre><code class="language-bash=">[ðŸŽ©ï¸Ž]mrajanna@fedora examples $]kubectl get cm rook-ceph-csi-config -nrook-ceph -oyaml
apiVersion: v1
data:
  csi-cluster-config-json: '[{"clusterID":"rook-ceph-external","monitors":["192.168.122.243:6789"],"namespace":"rook-ceph-external"},{"clusterID":"728ed87ac1a8809431b4549f4a4897aa","monitors":["192.168.122.243:6789"],"namespace":"rook-ceph-external","cephFS":{"subvolumeGroup":"volgroup"}}]'
kind: ConfigMap
metadata:
  creationTimestamp: "2022-06-10T07:57:13Z"
  name: rook-ceph-csi-config
  namespace: rook-ceph
  ownerReferences:
  - apiVersion: apps/v1
    blockOwnerDeletion: false
    controller: true
    kind: Deployment
    name: rook-ceph-operator
    uid: 5c8f3d9a-d973-4b65-b3f3-edbee10cd6b3
  resourceVersion: "5973"
  uid: 7ac9d0d9-a9ef-4e04-b322-63d01513c072
</code></pre>

<h4 id="create-storageclass-pvc-and-pod">Create StorageClass, PVC and Pod</h4>

<pre><code class="language-bash=">$cat &lt;&lt;EOF | kubectl apply -f -
allowVolumeExpansion: true
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: rook-cephfs-subvol-group
parameters:
  clusterID: 728ed87ac1a8809431b4549f4a4897aa # Subvolumegroup clusterID
  csi.storage.k8s.io/controller-expand-secret-name: rook-csi-cephfs-provisioner
  csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph-external # Namespace where ceph cluster is deployed
  csi.storage.k8s.io/node-stage-secret-name: rook-csi-cephfs-node
  csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph-external # Namespace where ceph cluster is deployed
  csi.storage.k8s.io/provisioner-secret-name: rook-csi-cephfs-provisioner
  csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph-external # Namespace where ceph cluster is deployed
  fsName: myfs
  pool: myfs-data
provisioner: rook-ceph.cephfs.csi.ceph.com
reclaimPolicy: Delete
volumeBindingMode: Immediate
EOF
storageclass.storage.k8s.io/rook-cephfs-subvol-group created
</code></pre>

<pre><code class="language-bash=">[ðŸŽ©ï¸Ž]mrajanna@fedora cephfs $]kubectl create -f pvc.yaml
persistentvolumeclaim/cephfs-subvol-pvc created
[ðŸŽ©ï¸Ž]mrajanna@fedora cephfs $]kubectl get pvc
NAME                STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS               AGE
cephfs-subvol-pvc   Bound    pvc-556acc86-ba24-43d6-ae19-85fa21a046a4   1Gi        RWO            rook-cephfs-subvol-group   2s
[ðŸŽ©ï¸Ž]mrajanna@fedora cephfs $]kubectl create -f pod.yaml
pod/csicephfssubvol-demo-pod created
[ðŸŽ©ï¸Ž]mrajanna@fedora cephfs $]kubectl get po
NAME                       READY   STATUS    RESTARTS   AGE
csicephfssubvol-demo-pod   1/1     Running   0          70s
</code></pre>

<p><strong>Note</strong>. Dont forget to change storageclass in the PVC yaml to point to
subvolumegroup storageClass</p>

<p>Letâ€™s verify we have cephfs Subvolume in the specified group created in the
ceph cluster.</p>

<pre><code class="language-bash=">sh-4.4$ ceph fs subvolume ls myfs --group_name=volgroup
[
    {
        "name": "csi-vol-e0905660-e898-11ec-93d2-8a68b48559cc"
    }
]
</code></pre>

<p><strong>Note</strong> <code class="language-plaintext highlighter-rouge">volgroup</code> is the SubVolumeGroup we created.</p>

<h2 id="radosnamespaces">RadosNamespaces</h2>

<p>To create RBD PVC in a RadosNamespaces, we need to first have RadosNamespaces
created in the RBD Pool created in the ceph cluster and ceph users to access
the RadosNamespaces and a StorageClass to point to the clusterID of
RadosNamespaces where to create the PVC.</p>

<p><em>Note</em> Above we have RadosNamespaces with the name <code class="language-plaintext highlighter-rouge">testnamespace</code> in the
<code class="language-plaintext highlighter-rouge">replicapool</code> Pool, Letâ€™s use the same for testing</p>

<h3 id="create-radosnamespaces-cr">Create RadosNamespaces CR</h3>

<pre><code class="language-bash=">[ðŸŽ©ï¸Ž]mrajanna@fedora examples $]cat &lt;&lt;EOF | kubectl apply -f -
---
apiVersion: ceph.rook.io/v1
kind: CephBlockPoolRadosNamespace
metadata:
  name: testnamespace
  namespace: rook-ceph-external
spec:
  blockPoolName: replicapool
EOF
cephblockpoolradosnamespace.ceph.rook.io/testnamespace created
</code></pre>

<p><strong>Note</strong> We are creating RadosNamespace CR to generate the RadosNamespace Name
and ClusterID mapping, it wonâ€™t create RadosNamespace in the Ceph cluster.</p>

<h4 id="get-radosnamespaces-details">Get RadosNamespaces details</h4>

<pre><code class="language-bash=">kubectl get cephblockcephblockpoolradosnamespace.ceph.rook.io/testnamespace -nrook-ceph-external
NAME            AGE
testnamespace   28s
[ðŸŽ©ï¸Ž]mrajanna@fedora examples $]kubectl get cephblockpoolradosnamespace.ceph.rook.io/testnamespace -nrook-ceph-external -o jsonpath='{.status.info}'
{"clusterID":"6165a9e9a61346a8b8e629bdfb583414"}
</code></pre>

<p>The above clusterID <code class="language-plaintext highlighter-rouge">6165a9e9a61346a8b8e629bdfb583414</code> is the one we need to
use it in RadosNamespace storageclass.</p>

<p>The reason we have a new clusterID for RadosNamespace is we cannot specify
RadosNamespace name in the storageclass, This clusterID has the mapping to the
RadosNamespace</p>

<h4 id="check-mapping-between-clusterid-and-radosnamespace">Check mapping between clusterID and RadosNamespace</h4>

<pre><code class="language-bash=">[ðŸŽ©ï¸Ž]mrajanna@fedora examples $]kubectl get cm rook-ceph-csi-config -nrook-ceph -oyaml
apiVersion: v1
data:
  csi-cluster-config-json: '[{"clusterID":"rook-ceph-external","monitors":["192.168.122.243:6789"],"namespace":"rook-ceph-external"},{"clusterID":"728ed87ac1a8809431b4549f4a4897aa","monitors":["192.168.122.243:6789"],"namespace":"rook-ceph-external","cephFS":{"subvolumeGroup":"volgroup"}},{"clusterID":"6165a9e9a61346a8b8e629bdfb583414","monitors":["192.168.122.243:6789"],"namespace":"rook-ceph-external","rbd":{"radosNamespace":"testnamespace"}}]'
kind: ConfigMap
metadata:
  creationTimestamp: "2022-06-10T07:57:13Z"
  name: rook-ceph-csi-config
  namespace: rook-ceph
  ownerReferences:
  - apiVersion: apps/v1
    blockOwnerDeletion: false
    controller: true
    kind: Deployment
    name: rook-ceph-operator
    uid: 5c8f3d9a-d973-4b65-b3f3-edbee10cd6b3
  resourceVersion: "7913"
  uid: 7ac9d0d9-a9ef-4e04-b322-63d01513c072

</code></pre>

<h4 id="create-storageclass-pvc-and-pod-1">Create StorageClass, PVC, and Pod</h4>

<pre><code class="language-bash=">$cat &lt;&lt;EOF | kubectl apply -f -
allowVolumeExpansion: true
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: rook-blockpool-radosnamespace
parameters:
  clusterID: 6165a9e9a61346a8b8e629bdfb583414 # RadosNamespace clusterID
  csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
  csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph-external
  csi.storage.k8s.io/fstype: ext4
  csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
  csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph-external
  csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
  csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph-external
  imageFeatures: layering
  imageFormat: "2"
  pool: replicapool
provisioner: rook-ceph.rbd.csi.ceph.com
reclaimPolicy: Delete
volumeBindingMode: Immediate
EOF
storageclass.storage.k8s.io/rook-blockpool-radosnamespace created
</code></pre>

<pre><code class="language-bash=">[ðŸŽ©ï¸Ž]mrajanna@fedora rbd $]kubectl create -f pvc.yaml
persistentvolumeclaim/rbd-radosnamespace-pvc created
[ðŸŽ©ï¸Ž]mrajanna@fedora rbd $]kubectl get pvc
NAME                     STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS                    AGE
rbd-radosnamespace-pvc   Bound    pvc-e664ea28-c625-41d3-aa81-aead7449e989   1Gi        RWO            rook-blockpool-radosnamespace   3s
[ðŸŽ©ï¸Ž]mrajanna@fedora rbd $]kubectl create -f pod.yaml
pod/csirbd-namespace-demo-pod created
[ðŸŽ©ï¸Ž]mrajanna@fedora rbd $]kubectl get po
NAME                        READY   STATUS    RESTARTS   AGE
csirbd-namespace-demo-pod   1/1     Running   0          26s

</code></pre>

<p><strong>Note</strong>. Dont forget to change storageclass in the PVC yaml to point to
RadosNamespace storageClass</p>

<p>Letâ€™s verify we have the rbd image created in the specified radosnamespace in
the ceph cluster.</p>

<pre><code class="language-bash=">sh-4.4$ rbd ls replicapool/testnamespace
csi-vol-3ca55f72-e89a-11ec-bbf0-9a1ab8207c3a
</code></pre>

<p>Yay!, We have created StorageClassâ€™s, PVCâ€™s, Podâ€™s and verfied for all the ceph
resources listed above.</p>

  </div>

  <div class="date">
    Written on June 10, 2022
  </div>
</article>


  <div id="disqus_thread"></div>
  <script>
    var disqus_config = function () {
      this.page.url = 'https://mrajanna.com/Consuming-storage-from-exteranal-ceph-in-rook/';
      this.page.identifier = 'https://mrajanna.com/Consuming-storage-from-exteranal-ceph-in-rook/';
    };
    (function() {
      var d = document, s = d.createElement('script');
      s.src = 'https://madhu-1-github-io-1.disqus.com/embed.js';
      s.setAttribute('data-timestamp', +new Date());
      (d.head || d.body).appendChild(s);
    })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>



    </div>

    <div class="wrapper-footer">
      <div class="container">
        <footer class="footer">
          
<a href="mailto:Madhupr007@gmail.com"><i class="svg-icon email"></i></a>


<a href="https://github.com/Madhu-1"><i class="svg-icon github"></i></a>

<a href="https://www.linkedin.com/in/madhu-r-4a551573"><i class="svg-icon linkedin"></i></a>


<a href="https://www.twitter.com/Madhupr007"><i class="svg-icon twitter"></i></a>



        </footer>
      </div>
    </div>

    
	<!-- Google Analytics -->
	<script>
		(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
		(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
		m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
		})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

		ga('create', 'qrFYIlFxt1yD_oRmFPV2ZpEOYn46EXVxnRpg2HhMaUU', 'auto');
		ga('send', 'pageview', {
		  'page': '/Consuming-storage-from-exteranal-ceph-in-rook/',
		  'title': 'Consuming Storage From external ceph cluster in Rook'
		});
	</script>
	<!-- End Google Analytics -->


  </body>
</html>
