<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://mrajanna.com/feed.xml" rel="self" type="application/atom+xml" /><link href="https://mrajanna.com/" rel="alternate" type="text/html" /><updated>2025-07-11T10:38:38+02:00</updated><id>https://mrajanna.com/feed.xml</id><title type="html">Madhu Rajanna</title><subtitle>Senior Software Engineer at RedHat</subtitle><entry><title type="html">Adding Support for Custom clusterID in Rook Ceph CSI</title><link href="https://mrajanna.com/custom-clusterID-in-rook/" rel="alternate" type="text/html" title="Adding Support for Custom clusterID in Rook Ceph CSI" /><published>2025-07-10T00:00:00+02:00</published><updated>2025-07-10T00:00:00+02:00</updated><id>https://mrajanna.com/custom-clusterID-in-rook</id><content type="html" xml:base="https://mrajanna.com/custom-clusterID-in-rook/"><![CDATA[<p>In the latest enhancement to Rook‚Äôs Ceph operator, users can now explicitly specify the <code class="language-plaintext highlighter-rouge">clusterID</code> when creating <code class="language-plaintext highlighter-rouge">CephBlockPoolRadosNamespace</code> and <code class="language-plaintext highlighter-rouge">CephFilesystemSubVolumeGroup</code> custom resources. This update gives users more control and flexibility over how storage clusters are identified and referenced by the CSI driver.</p>

<h2 id="-why-this-feature-matters">üö® Why This Feature Matters</h2>

<p>Previously, the <code class="language-plaintext highlighter-rouge">clusterID</code> was internally generated by the <strong>Rook operator</strong>, meaning users had no influence over how it was defined or named. The users need to create the <code class="language-plaintext highlighter-rouge">CephBlockPoolRadosNamespace</code> and <code class="language-plaintext highlighter-rouge">CephFilesystemSubVolumeGroup</code> CR‚Äôs and wait for <code class="language-plaintext highlighter-rouge">clusterID</code> to appear in the status of these CR‚Äôs and then later create the StorageClass and consume it.</p>

<p>With this change:</p>

<ul>
  <li>The <code class="language-plaintext highlighter-rouge">clusterID</code> can now be explicitly <strong>defined by the user</strong>.</li>
  <li>Users can assign <strong>meaningful and recognizable names</strong> to their clusters (e.g., <code class="language-plaintext highlighter-rouge">prod-ceph</code>, <code class="language-plaintext highlighter-rouge">backup-ceph</code>, <code class="language-plaintext highlighter-rouge">test-cephfs-clusterid</code>).</li>
  <li>This also helps to <strong>avoid hard-to-debug mismatches</strong> between the CSI config and Ceph cluster references.</li>
</ul>

<h2 id="-important-note">üìù Important Note</h2>

<blockquote>
  <p>‚ö†Ô∏è It is the <strong>user‚Äôs responsibility</strong> to ensure the <code class="language-plaintext highlighter-rouge">clusterID</code> is <strong>unique across all CephClusters</strong> managed by the same Rook operator instance.</p>
</blockquote>

<p>Duplicate or conflicting <code class="language-plaintext highlighter-rouge">clusterID</code>s can result in <strong>unexpected CSI behavior</strong>, incorrect volume provisioning, or failures.</p>

<h2 id="-how-to-use-the-new-clusterid-field">üîß How to Use the New <code class="language-plaintext highlighter-rouge">clusterID</code> Field</h2>

<h3 id="cephblockpoolradosnamespace-example">CephBlockPoolRadosNamespace Example</h3>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">ceph.rook.io/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">CephBlockPoolRadosNamespace</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">namespace-a</span>
  <span class="na">namespace</span><span class="pi">:</span> <span class="s">rook-ceph</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">blockPoolName</span><span class="pi">:</span> <span class="s">replicapool</span>
  <span class="na">clusterID</span><span class="pi">:</span> <span class="s">rbd-test-clusterid</span>
</code></pre></div></div>

<h3 id="cephfilesystemsubvolumegroup-example">CephFilesystemSubVolumeGroup Example</h3>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">ceph.rook.io/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">CephFilesystemSubVolumeGroup</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">group-a</span>
  <span class="na">namespace</span><span class="pi">:</span> <span class="s">rook-ceph</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">filesystemName</span><span class="pi">:</span> <span class="s">myfs</span>
  <span class="na">dataPoolName</span><span class="pi">:</span> <span class="s2">"</span><span class="s">"</span>
  <span class="na">pinning</span><span class="pi">:</span>
    <span class="na">distributed</span><span class="pi">:</span> <span class="m">1</span>
  <span class="na">clusterID</span><span class="pi">:</span> <span class="s">cephfs-test-clusterid</span>
</code></pre></div></div>

<p>Once you create above CR‚Äôs backend resouces will be created for it and Rook update the status with Ready state and with
the clusterID specified in the spec field.</p>

<h3 id="-storageclass-examples">üì¶ StorageClass Examples</h3>

<p>Using above ClusterID we can create the StorageClass and provisioning PVC from it.</p>

<h4 id="-rbd-storageclass">üî∑ RBD StorageClass</h4>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">storage.k8s.io/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">StorageClass</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">rbd-sc</span>
<span class="na">provisioner</span><span class="pi">:</span> <span class="s">rook-ceph.rbd.csi.ceph.com</span>
<span class="na">parameters</span><span class="pi">:</span>
  <span class="na">clusterID</span><span class="pi">:</span> <span class="s">rbd-test-clusterid</span>
  <span class="s">...</span>
<span class="na">reclaimPolicy</span><span class="pi">:</span> <span class="s">Delete</span>
<span class="na">allowVolumeExpansion</span><span class="pi">:</span> <span class="kc">true</span>
</code></pre></div></div>

<h4 id="-cephfs-storageclass">üî∑ CephFS StorageClass</h4>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">storage.k8s.io/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">StorageClass</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">cephfs-sc</span>
<span class="na">provisioner</span><span class="pi">:</span> <span class="s">rook-ceph.cephfs.csi.ceph.com</span>
<span class="na">parameters</span><span class="pi">:</span>
  <span class="na">clusterID</span><span class="pi">:</span> <span class="s">cephfs-test-clusterid</span>
  <span class="s">...</span>
<span class="na">reclaimPolicy</span><span class="pi">:</span> <span class="s">Delete</span>
<span class="na">allowVolumeExpansion</span><span class="pi">:</span> <span class="kc">true</span>
</code></pre></div></div>

<p>‚úÖ Summary</p>

<p>This new clusterID support puts control back into the hands of cluster administrators. It simplifies CSI integration, makes configurations more predictable.</p>

<p>Try it out and structure your CSI configuration the way you want it!</p>]]></content><author><name></name></author><category term="ceph" /><category term="rook" /><category term="storage" /><category term="rook" /><category term="ceph" /><category term="kubernetes" /><category term="storage" /><category term="csi" /><summary type="html"><![CDATA[In the latest enhancement to Rook‚Äôs Ceph operator, users can now explicitly specify the clusterID when creating CephBlockPoolRadosNamespace and CephFilesystemSubVolumeGroup custom resources. This update gives users more control and flexibility over how storage clusters are identified and referenced by the CSI driver.]]></summary></entry><entry><title type="html">Consuming Storage From external ceph cluster in Rook</title><link href="https://mrajanna.com/Consuming-storage-from-exteranal-ceph-in-rook/" rel="alternate" type="text/html" title="Consuming Storage From external ceph cluster in Rook" /><published>2022-06-10T00:00:00+02:00</published><updated>2022-06-10T00:00:00+02:00</updated><id>https://mrajanna.com/Consuming-storage-from-exteranal-ceph-in-rook</id><content type="html" xml:base="https://mrajanna.com/Consuming-storage-from-exteranal-ceph-in-rook/"><![CDATA[This blog will help you to Understand how to consume storage from external ceph
cluster in Rook and how to create and consume Below listed resources in the
ceph cluster.

This doc assumes that you already set up the Rook with an external ceph cluster.

Please refer to [doc](../setup-external-ceph-cluster-with-rook) on how to setup
external ceph cluster with Rook.

* CephBlockPool
* CephFileSystem
* SubvolumeGroup
* RadosNamespace

## Create resources specified above in the ceph cluster

**Note** You can skip this step and create the ceph resources with Rook CRs if
you are using the ceph cluster deployed with Rook.

### Create RBD Pool

```bash
[üé©Ô∏é]mrajanna@fedora $]ceph osd pool create replicapool 128
[üé©Ô∏é]mrajanna@fedora $]rbd pool init
[üé©Ô∏é]mrajanna@fedora $]ceph osd pool set replicapool size 1 --yes-i-really-mean-it
```

### Create CephFS Filesystem

```bash
[üé©Ô∏é]mrajanna@fedora $]ceph osd pool create myfs-metadata 128
[üé©Ô∏é]mrajanna@fedora $]ceph osd pool create myfs-data 124
[üé©Ô∏é]mrajanna@fedora $]ceph fs new myfs myfs-metadata myfs-data
```

### Create SubVolumeGroup

Let's create a SubVolumeGroup in the Filesystem `myfs` created above

```bash
[üé©Ô∏é]mrajanna@fedora $]ceph fs subvolumegroup create myfs volgroup
```

### Create RadosNamespace

Let's create a RadosNamespace in the BlockPool `replicapool` created above

```bash
[üé©Ô∏é]mrajanna@fedora $]rbd namespace create replicapool/testnamespace
```

## checkout Rook v1.9.5 Repository

```bash
[üé©Ô∏é]mrajanna@fedora $]git clone https://github.com/rook/rook.git
[üé©Ô∏é]mrajanna@fedora $]git checkout v1.9.5
[üé©Ô∏é]mrajanna@fedora $]cd rook/deploy/examples
```

## Consume above created ceph resources in kubernetes cluster

### CephBlockPool

To create RBD PVC, we need to first have the RBD pool created in the ceph
cluster and ceph users to access the pool and a StorageClass to point to the
Pool where to create the PVC.

*Note* Above create a pool with the name `replicapool` with  PG Num `128` and
setting replica size to `1` As I have only one OSD. If you have a minimum `3`
OSD please set it to `3`.

As this ceph cluster is deployed in the `rook-ceph-external` namespace, we need
to update a few details in RBD storageclass

```bash
[üé©Ô∏é]mrajanna@fedora $] cd csi/rbd
[üé©Ô∏é]mrajanna@fedora $] cat storageclass-test.yaml
allowVolumeExpansion: true
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: rook-ceph-block
parameters:
  clusterID: rook-ceph-external  # Namespace where ceph cluster is deployed
  csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
  csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph-external  # Namespace where ceph cluster is deployed
  csi.storage.k8s.io/fstype: ext4
  csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
  csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph-external  # Namespace where ceph cluster is deployed
  csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
  csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph-external  # Namespace where ceph cluster is deployed
  imageFeatures: layering
  imageFormat: "2"
  pool: replicapool
provisioner: rook-ceph.rbd.csi.ceph.com
reclaimPolicy: Delete
volumeBindingMode: Immediate
```

change the above parameters in the rbd storageclass-test.yaml and keep all
other configurations as it is (Dont forget to remove pool.yaml from
storageclass-test.yaml).

Let's create RBD storageclass, PVC, Pod and verify it on the ceph cluster.

```bash
[üé©Ô∏é]mrajanna@fedora $]kubectl create -f storageclass-test.yaml
storageclass.storage.k8s.io/rook-ceph-block created
[üé©Ô∏é]mrajanna@fedora $]kubectl create -f pvc.yaml
persistentvolumeclaim/rbd-pvc created
[üé©Ô∏é]mrajanna@fedora $]kubectl get pvc
NAME      STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS      AGE
rbd-pvc   Bound    pvc-a1a802fd-a577-45e4-b52d-687b8333cbef   1Gi        RWO            rook-ceph-block   4s
[üé©Ô∏é]mrajanna@fedora $]kubectl create -f pod.yaml
pod/csirbd-demo-pod created
[üé©Ô∏é]mrajanna@fedora $]kubectl get po
NAME              READY   STATUS    RESTARTS   AGE
csirbd-demo-pod   1/1     Running   0          43s
```

Let's verify we have the rbd image in the ceph cluster.

```bash
[üé©Ô∏é]mrajanna@fedora $] rbd ls replicapool
csi-vol-9475df3e-e895-11ec-bbf0-9a1ab8207c3a
```

### CephFilesystem

To create CephFS PVC, we need to first have the CephFS Filesystem created in
the ceph cluster and ceph users to access the Filesystem and a StorageClass to
point to the Filesystem where to create the PVC.

*Note* Above a Filesystem with the name `myfs`, let's use the same for testing

As this ceph cluster is deployed in the `rook-ceph-external` namespace, we need
to update a few details in CephFS storageclass

```bash
[üé©Ô∏é]mrajanna@fedora $] cd csi/cephfs
[üé©Ô∏é]mrajanna@fedora $] cat storageclass.yaml
allowVolumeExpansion: true
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: rook-cephfs
parameters:
  clusterID: rook-ceph-external # Namespace where ceph cluster is deployed
  csi.storage.k8s.io/controller-expand-secret-name: rook-csi-cephfs-provisioner
  csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph-external # Namespace where ceph cluster is deployed
  csi.storage.k8s.io/node-stage-secret-name: rook-csi-cephfs-node
  csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph-external # Namespace where ceph cluster is deployed
  csi.storage.k8s.io/provisioner-secret-name: rook-csi-cephfs-provisioner
  csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph-external # Namespace where ceph cluster is deployed
  fsName: myfs
  pool: myfs-data
provisioner: rook-ceph.cephfs.csi.ceph.com
reclaimPolicy: Delete
volumeBindingMode: Immediate
```

change the above parameters in the cephfs storageclass.yaml and keep all other
configurations as it is.

Let's create CephFS storageclass, PVC, Pod and verify it on the ceph cluster.

```bash
[üé©Ô∏é]mrajanna@fedora $]kubectl create  -f cephfs/storageclass.yaml
storageclass.storage.k8s.io/rook-cephfs created
[üé©Ô∏é]mrajanna@fedora $]kubectl create -f pvc.yaml
persistentvolumeclaim/cephfs-pvc created
[üé©Ô∏é]mrajanna@fedora $]kubectl get pvc
NAME         STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS      AGE
cephfs-pvc   Bound    pvc-52186bd0-92dd-433c-af00-e0a60baf71a1   1Gi        RWO            rook-cephfs       2s
[üé©Ô∏é]mrajanna@fedora $]kubectl create -f pod.yaml
pod/csicephfs-demo-pod created
[üé©Ô∏é]mrajanna@fedora $]kubectl get po
NAME                 READY   STATUS    RESTARTS   AGE
csicephfs-demo-pod   1/1     Running   0          24s
```

Let's verify we have cephfs Subvolume created in the ceph cluster.

```bash
[üé©Ô∏é]mrajanna@fedora $] ceph fs subvolume ls myfs --group_name=csi
[
    {
        "name": "csi-vol-9bcadef0-e896-11ec-93d2-8a68b48559cc"
    }
]
```

**Note** `csi` is the default subvolumegroup cephcsi creates.

### SubvolumeGroup

To create CephFS PVC in a subvolumeGroup, we need to first have subvolumegroup
created in the CephFS Filesystem created in the ceph cluster and ceph users to
access the subvolumegroup and a StorageClass to point to the clusterID of
subvolumegroup where to create the PVC.

*Note* Above we have subvolumegroup with name `volgroup` in `myfs` Filesystem,
Let's use the same for testing

#### Create SubVolumeGroup CR

```bash
[üé©Ô∏é]mrajanna@fedora $]cat <<EOFcat <<EOF | kubectl apply -f -
---
apiVersion: ceph.rook.io/v1
kind: CephFilesystemSubVolumeGroup
metadata:
  name: volgroup
  namespace: rook-ceph-external
spec:
  filesystemName: myfs
EOF
cephfilesystemsubvolumegroup.ceph.rook.io/volgroup created
```

**Note** We are creating SubVolumeGroup CR to generate the SubVolumeGroup Name
and ClusterID mapping, it won't create SubVolumeGroup in the Ceph cluster.

#### Check SubVolumeGroup details

```bash
[üé©Ô∏é]mrajanna@fedora $]kubectl get cephfilesystemsubvolumegroup.ceph.rook.io/volgroup -nrook-ceph-external
NAME       PHASE
volgroup   Ready
[üé©Ô∏é]mrajanna@fedora $]kubectl get cephfilesystemsubvolumegroup.ceph.rook.io/volgroup -nrook-ceph-external -o jsonpath='{.status.info}'
{"clusterID":"728ed87ac1a8809431b4549f4a4897aa"}
```

The above clusterID `728ed87ac1a8809431b4549f4a4897aa` is the one we need to
use it in subvolumegroup storageclass.

The reason we have a new clusterID for subvolumegroup is we cannot specify
subvolumegroup name in the storageclass, This clusterID has the mapping to the
subvolumegroup

#### Check mapping between clusterID and SubVolumeGroup

```bash
[üé©Ô∏é]mrajanna@fedora $]kubectl get cm rook-ceph-csi-config -nrook-ceph -oyaml
apiVersion: v1
data:
  csi-cluster-config-json: '[{"clusterID":"rook-ceph-external","monitors":["192.168.122.243:6789"],"namespace":"rook-ceph-external"},{"clusterID":"728ed87ac1a8809431b4549f4a4897aa","monitors":["192.168.122.243:6789"],"namespace":"rook-ceph-external","cephFS":{"subvolumeGroup":"volgroup"}}]'
kind: ConfigMap
metadata:
  creationTimestamp: "2022-06-10T07:57:13Z"
  name: rook-ceph-csi-config
  namespace: rook-ceph
  ownerReferences:
  - apiVersion: apps/v1
    blockOwnerDeletion: false
    controller: true
    kind: Deployment
    name: rook-ceph-operator
    uid: 5c8f3d9a-d973-4b65-b3f3-edbee10cd6b3
  resourceVersion: "5973"
  uid: 7ac9d0d9-a9ef-4e04-b322-63d01513c072
```

#### Create StorageClass, PVC and Pod

```bash
[üé©Ô∏é]mrajanna@fedora $]cat <<EOF | kubectl apply -f -
allowVolumeExpansion: true
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: rook-cephfs-subvol-group
parameters:
  clusterID: 728ed87ac1a8809431b4549f4a4897aa # Subvolumegroup clusterID
  csi.storage.k8s.io/controller-expand-secret-name: rook-csi-cephfs-provisioner
  csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph-external # Namespace where ceph cluster is deployed
  csi.storage.k8s.io/node-stage-secret-name: rook-csi-cephfs-node
  csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph-external # Namespace where ceph cluster is deployed
  csi.storage.k8s.io/provisioner-secret-name: rook-csi-cephfs-provisioner
  csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph-external # Namespace where ceph cluster is deployed
  fsName: myfs
  pool: myfs-data
provisioner: rook-ceph.cephfs.csi.ceph.com
reclaimPolicy: Delete
volumeBindingMode: Immediate
EOF
storageclass.storage.k8s.io/rook-cephfs-subvol-group created
```

```bash
[üé©Ô∏é]mrajanna@fedora $]kubectl create -f pvc.yaml
persistentvolumeclaim/cephfs-subvol-pvc created
[üé©Ô∏é]mrajanna@fedora $]kubectl get pvc
NAME                STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS               AGE
cephfs-subvol-pvc   Bound    pvc-556acc86-ba24-43d6-ae19-85fa21a046a4   1Gi        RWO            rook-cephfs-subvol-group   2s
[üé©Ô∏é]mrajanna@fedora $]kubectl create -f pod.yaml
pod/csicephfssubvol-demo-pod created
[üé©Ô∏é]mrajanna@fedora $]kubectl get po
NAME                       READY   STATUS    RESTARTS   AGE
csicephfssubvol-demo-pod   1/1     Running   0          70s
```

**Note**. Dont forget to change storageclass in the PVC yaml to point to
subvolumegroup storageClass

Let's verify we have cephfs Subvolume in the specified group created in the
ceph cluster.

```bash
[üé©Ô∏é]mrajanna@fedora $]ceph fs subvolume ls myfs --group_name=volgroup
[
    {
        "name": "csi-vol-e0905660-e898-11ec-93d2-8a68b48559cc"
    }
]
```

**Note** `volgroup` is the SubVolumeGroup we created.

## RadosNamespaces

To create RBD PVC in a RadosNamespaces, we need to first have RadosNamespaces
created in the RBD Pool created in the ceph cluster and ceph users to access
the RadosNamespaces and a StorageClass to point to the clusterID of
RadosNamespaces where to create the PVC.

*Note* Above we have RadosNamespaces with the name `testnamespace` in the
`replicapool` Pool, Let's use the same for testing

### Create RadosNamespaces CR

```bash
[üé©Ô∏é]mrajanna@fedora $]cat <<EOF | kubectl apply -f -
---
apiVersion: ceph.rook.io/v1
kind: CephBlockPoolRadosNamespace
metadata:
  name: testnamespace
  namespace: rook-ceph-external
spec:
  blockPoolName: replicapool
EOF
cephblockpoolradosnamespace.ceph.rook.io/testnamespace created
```

**Note** We are creating RadosNamespace CR to generate the RadosNamespace Name
and ClusterID mapping, it won't create RadosNamespace in the Ceph cluster.

#### Get RadosNamespaces details

```bash
[üé©Ô∏é]mrajanna@fedora $]kubectl get cephblockcephblockpoolradosnamespace.ceph.rook.io/testnamespace -nrook-ceph-external
NAME            AGE
testnamespace   28s
[üé©Ô∏é]mrajanna@fedora $]kubectl get cephblockpoolradosnamespace.ceph.rook.io/testnamespace -nrook-ceph-external -o jsonpath='{.status.info}'
{"clusterID":"6165a9e9a61346a8b8e629bdfb583414"}
```

The above clusterID `6165a9e9a61346a8b8e629bdfb583414` is the one we need to
use it in RadosNamespace storageclass.

The reason we have a new clusterID for RadosNamespace is we cannot specify
RadosNamespace name in the storageclass, This clusterID has the mapping to the
RadosNamespace

#### Check mapping between clusterID and RadosNamespace

```bash
[üé©Ô∏é]mrajanna@fedora $]kubectl get cm rook-ceph-csi-config -nrook-ceph -oyaml
apiVersion: v1
data:
  csi-cluster-config-json: '[{"clusterID":"rook-ceph-external","monitors":["192.168.122.243:6789"],"namespace":"rook-ceph-external"},{"clusterID":"728ed87ac1a8809431b4549f4a4897aa","monitors":["192.168.122.243:6789"],"namespace":"rook-ceph-external","cephFS":{"subvolumeGroup":"volgroup"}},{"clusterID":"6165a9e9a61346a8b8e629bdfb583414","monitors":["192.168.122.243:6789"],"namespace":"rook-ceph-external","rbd":{"radosNamespace":"testnamespace"}}]'
kind: ConfigMap
metadata:
  creationTimestamp: "2022-06-10T07:57:13Z"
  name: rook-ceph-csi-config
  namespace: rook-ceph
  ownerReferences:
  - apiVersion: apps/v1
    blockOwnerDeletion: false
    controller: true
    kind: Deployment
    name: rook-ceph-operator
    uid: 5c8f3d9a-d973-4b65-b3f3-edbee10cd6b3
  resourceVersion: "7913"
  uid: 7ac9d0d9-a9ef-4e04-b322-63d01513c072

```

#### Create StorageClass, PVC, and Pod

```bash
[üé©Ô∏é]mrajanna@fedora $]cat <<EOF | kubectl apply -f -
allowVolumeExpansion: true
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: rook-blockpool-radosnamespace
parameters:
  clusterID: 6165a9e9a61346a8b8e629bdfb583414 # RadosNamespace clusterID
  csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
  csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph-external
  csi.storage.k8s.io/fstype: ext4
  csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
  csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph-external
  csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
  csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph-external
  imageFeatures: layering
  imageFormat: "2"
  pool: replicapool
provisioner: rook-ceph.rbd.csi.ceph.com
reclaimPolicy: Delete
volumeBindingMode: Immediate
EOF
storageclass.storage.k8s.io/rook-blockpool-radosnamespace created
```

```bash
[üé©Ô∏é]mrajanna@fedora $]kubectl create -f pvc.yaml
persistentvolumeclaim/rbd-radosnamespace-pvc created
[üé©Ô∏é]mrajanna@fedora $]kubectl get pvc
NAME                     STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS                    AGE
rbd-radosnamespace-pvc   Bound    pvc-e664ea28-c625-41d3-aa81-aead7449e989   1Gi        RWO            rook-blockpool-radosnamespace   3s
[üé©Ô∏é]mrajanna@fedora $]kubectl create -f pod.yaml
pod/csirbd-namespace-demo-pod created
[üé©Ô∏é]mrajanna@fedora $]kubectl get po
NAME                        READY   STATUS    RESTARTS   AGE
csirbd-namespace-demo-pod   1/1     Running   0          26s

```

**Note**. Dont forget to change storageclass in the PVC yaml to point to
RadosNamespace storageClass

Let's verify we have the rbd image created in the specified radosnamespace in
the ceph cluster.

```bash
[üé©Ô∏é]mrajanna@fedora $]rbd ls replicapool/testnamespace
csi-vol-3ca55f72-e89a-11ec-bbf0-9a1ab8207c3a
```

Yay!, We have created StorageClass's, PVC's, Pod's and verfied for all the ceph
resources listed above.]]></content><author><name></name></author><summary type="html"><![CDATA[This blog will help you to Understand how to consume storage from external ceph cluster in Rook and how to create and consume Below listed resources in the ceph cluster.]]></summary></entry><entry><title type="html">DR failover and failback for RBD Async mirroring with minikube</title><link href="https://mrajanna.com/rbd-async-mirroring-dr-with-rook/" rel="alternate" type="text/html" title="DR failover and failback for RBD Async mirroring with minikube" /><published>2021-05-04T00:00:00+02:00</published><updated>2021-05-04T00:00:00+02:00</updated><id>https://mrajanna.com/rbd-async-mirroring-dr-with-rook</id><content type="html" xml:base="https://mrajanna.com/rbd-async-mirroring-dr-with-rook/"><![CDATA[This doc assumes that you already set up the rbd mirroring between two clusters.

Please refer to [doc](../setup-rbd-async-mirroring-with-rook) on how
to setup RBD mirroring.

## Set up initial DR components

we need to Enable two new sidecar deployment in the RBD provisioner pods which
will help in achieving rbd mirroring

* OMAP Regenerator -> This is a cephcsi component that regenerates the
  required OMAP data for cephcsi to perform CSI operations after failover.

* Volume Replication operator -> Volume replication operator which exposes
  Replication CRD to perform the RBD async operations like enable and disable mirroring,
  promote, demote, force promote, and resync. Refer [operator](https://github.com/csi-addons/volume-replication-operator) for more details about it.

### Enable DR components

#### Upgrade to Rook v1.6.0+

Rook [v1.6.0](https://github.com/rook/rook/releases/tag/v1.6.0) comes with the
new volume replication support and also cephcsi v3.3.0, Install/Upgrade to Rook
v1.6.0 or higher versions.

#### Re-apply the common.yaml and crds.yaml

```bash=
kubectl apply -f crds.yaml common.yaml
```

> Once we reapply/create the crds.yaml we should see two new CRD's related to
> volume replication

```bash=
$ kubectl get crd |grep volumereplication
volumereplicationclasses.replication.storage.openshift.io   2021-04-28T05:57:57Z
volumereplications.replication.storage.openshift.io         2021-04-28T05:57:57Z
```

As we need to provide Edit `rook-ceph-operator-config` configmap and add below
two configurations to the list

```yaml
apiVersion: v1
data:
  CSI_ENABLE_OMAP_GENERATOR: "true"
  CSI_ENABLE_VOLUME_REPLICATION: "true"
```

```bash=
$kubectl edit cm rook-ceph-operator-config  -nrook-ceph
```

> Once you edit and save the configmap you will see that CSI pods are getting
> recreated, just wait until you see 8 containers in `csi-rbdplugin-provisioner-xxx`
> pod.

```bash=
kubectl get po -nrook-ceph
NAME                                            READY   STATUS      RESTARTS   AGE
csi-cephfsplugin-bkgkt                          3/3     Running     0          5d23h
csi-cephfsplugin-provisioner-6544c5b576-ndf58   6/6     Running     0          5d23h
csi-rbdplugin-provisioner-7465df5764-5g8mj      8/8     Running     0          6m7s
csi-rbdplugin-vpq4k                             3/3     Running     0          5d23h
rook-ceph-mgr-a-54bbfb54b9-pg827                1/1     Running     0          6d
rook-ceph-mon-a-79dfbdcdb6-qwssl                1/1     Running     0          6d
rook-ceph-operator-54cf7487d4-gdp5s             1/1     Running     0          6d
rook-ceph-osd-0-b99c76b7b-6cbnk                 1/1     Running     0          6d
rook-ceph-osd-prepare-minicluster2-57552        0/1     Completed   0          6d
rook-ceph-rbd-mirror-a-d449c66db-2c4hr          1/1     Running     0          5d23h
rook-ceph-tools-78cdfd976c-nm2jx                1/1     Running     0          6d
```

```bash=
kubectl logs po/csi-rbdplugin-provisioner-7465df5764-5g8mj -nrook-ceph
error: a container name must be specified for pod csi-rbdplugin-provisioner-7465df5764-5g8mj, choose one of: [csi-provisioner csi-resizer csi-attacher csi-snapshotter csi-omap-generator volume-replication csi-rbdplugin liveness-prometheus]
```

> In the above logs we can see that `csi-omap-generator` and
> `volume-replication` containers got created in
> `csi-rbdplugin-provisioner-xxx` pods.

**Note** The above steps need to be done on both clusters.

Now the initial bootstrapping is completed.

## Provision storage on cluster1

Create a `storageclass` and `PVC`

### Create storageclass on cluster1

```bash=
$cat <<EOF | kubectl --context=cluster1 apply -f -
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
   name: rook-ceph-block
provisioner: rook-ceph.rbd.csi.ceph.com
parameters:
    clusterID: rook-ceph
    pool: replicapool
    imageFormat: "2"
    imageFeatures: layering
    csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
    csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
    csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
    csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph
    csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
    csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph
    csi.storage.k8s.io/fstype: ext4
reclaimPolicy: Retain
EOF
storageclass.storage.k8s.io/rook-ceph-block created
```

> Create a storageclass with `Retain` `reclaimPolicy` so that we can use static binding for
DR.

### Create PersistentVolumeClaim on cluster1

```bash=
$cat <<EOF | kubectl --context=cluster1 apply -f -
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: rbd-pvc
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
  storageClassName: rook-ceph-block
EOF
```

Check PVC is in bound state

```bash=
$kubectl get pvc --context=cluster1
NAME      STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS      AGE
rbd-pvc   Bound    pvc-57cfca36-cb76-4d79-b258-a107bc5c4d15   1Gi        RWO            rook-ceph-block   44s
```

### Create Volume Replication class on cluster1

```bash=
$cat <<EOF | kubectl --context=cluster1 apply -f -
apiVersion: replication.storage.openshift.io/v1alpha1
kind: VolumeReplicationClass
metadata:
  name: rbd-volumereplicationclass
spec:
  provisioner: rook-ceph.rbd.csi.ceph.com
  parameters:
    mirroringMode: snapshot
    replication.storage.openshift.io/replication-secret-name: rook-csi-rbd-provisioner
    replication.storage.openshift.io/replication-secret-namespace: rook-ceph
EOF
```

> The volume replication class holds the storage admin information required
for the volume replication operator.

### Create Volume Replication for PVC on cluster1

```bash=
$cat <<EOF | kubectl --context=cluster1 apply -f -
apiVersion: replication.storage.openshift.io/v1alpha1
kind: VolumeReplication
metadata:
  name: pvc-volumereplication2
spec:
  volumeReplicationClass: rbd-volumereplicationclass
  replicationState: primary
  dataSource:
    apiGroup: ""
    kind: PersistentVolumeClaim
    name: rbd-pvc # Name of the PVC to which mirroring to be enabled.
EOF
```

>`replicationState` is the state of the volume being referenced.
  Possible values are `primary`, `secondary`, and `resync`.
* `primary` denotes that the volume is primary
* `secondary` denotes that the volume is secondary
* `resync` denotes that the volume needs to be resynced

> Note the `VolumeReplication` is a namespace scoped object it should be in the namespace as of PVC.

Check VolumeReplication CR status

```bash=
$ kubectl get volumereplication pvc-volumereplication -oyaml
...
spec:
  dataSource:
    apiGroup: ""
    kind: PersistentVolumeClaim
    name: rbd-pvc
  replicationState: primary
  volumeReplicationClass: rbd-volumereplicationclass
status:
  conditions:
  - lastTransitionTime: "2021-05-04T07:39:00Z"
    message: ""
    observedGeneration: 1
    reason: Promoted
    status: "True"
    type: Completed
  - lastTransitionTime: "2021-05-04T07:39:00Z"
    message: ""
    observedGeneration: 1
    reason: Healthy
    status: "False"
    type: Degraded
  - lastTransitionTime: "2021-05-04T07:39:00Z"
    message: ""
    observedGeneration: 1
    reason: NotResyncing
    status: "False"
    type: Resyncing
  lastCompletionTime: "2021-05-04T07:39:00Z"
  lastStartTime: "2021-05-04T07:38:59Z"
  message: volume is marked primary
  observedGeneration: 1
  state: Primary
```

Run commands on toolbox pod on cluster1 to check to mirror is enabled on ceph block pool

```bash=
$kubectl get po --context=cluster1 -nrook-ceph -l  app=rook-ceph-tools
NAME                               READY   STATUS    RESTARTS   AGE
rook-ceph-tools-78cdfd976c-jrhjm   1/1     Running   0          3m53s
```

Verify that mirroring enabled on the image

```bash=
$kubectl exec --context=cluster1 -nrook-ceph rook-ceph-tools-78cdfd976c-jrhjm -- rbd info csi-vol-1b82d349-2d5a-11eb-b8d5-0242ac110004 --pool=replicapool
rbd image 'csi-vol-1b82d349-2d5a-11eb-b8d5-0242ac110004':
    size 1 GiB in 256 objects
    order 22 (4 MiB objects)
    snapshot_count: 1
    id: 13a672b41f5f
    block_name_prefix: rbd_data.13a672b41f5f
    format: 2
    features: layering
    op_features:
    flags:
    create_timestamp: Mon Nov 23 07:04:21 2020
    access_timestamp: Mon Nov 23 07:04:21 2020
    modify_timestamp: Mon Nov 23 07:04:21 2020
    mirroring state: enabled
    mirroring mode: snapshot
    mirroring global id: 7da10ec2-aa67-4b05-86f0-487b4fa9fdbb
    mirroring primary: true
```

> You can get the RBD image name from the PV object.

#### Verify that the image is mirrored on the secondary cluster

Run commands on toolbox pod on cluster2

```bash=
$kubectl get po --context=cluster2 -nrook-ceph -l  app=rook-ceph-tools
NAME                               READY   STATUS    RESTARTS   AGE
rook-ceph-tools-78cdfd976c-2566m   1/1     Running   0          3m53s
```

```bash=
$kubectl exec --context=cluster2 -nrook-ceph rook-ceph-tools-78cdfd976c-2566m -- rbd info csi-vol-1b82d349-2d5a-11eb-b8d5-0242ac110004 --pool=replicapool
rbd image 'csi-vol-1b82d349-2d5a-11eb-b8d5-0242ac110004':
    size 1 GiB in 256 objects
    order 22 (4 MiB objects)
    snapshot_count: 1
    id: 12a55edc8361
    block_name_prefix: rbd_data.12a55edc8361
    format: 2
    features: layering, non-primary
    op_features:
    flags:
    create_timestamp: Mon Nov 23 07:12:33 2020
    access_timestamp: Mon Nov 23 07:12:33 2020
    modify_timestamp: Mon Nov 23 07:12:33 2020
    mirroring state: enabled
    mirroring mode: snapshot
    mirroring global id: 7da10ec2-aa67-4b05-86f0-487b4fa9fdbb
    mirroring primary: false
```

### Take a backup of PVC and PV object on cluster1

```bash=
kubectl get pvc rbd-pvc -oyaml >pvc-bkp.yaml
```

```bash=
kubectl get pv/pvc-0b93de3a-3946-4909-a189-f44dba0abc76 -oyaml >pv_bkp.yaml
```

#### Remove the claimRef on the backed up PV objects in  yaml files

sample claimRef in PV object

```yaml=
apiVersion: v1
kind: PersistentVolume
metadata:
  ...
  name: pvc-0b93de3a-3946-4909-a189-f44dba0abc76
  resourceVersion: "2516"
  selfLink: /api/v1/persistentvolumes/pvc-0b93de3a-3946-4909-a189-f44dba0abc76
  uid: 0b1a1560-2c0b-4906-8059-d6be95954bc0
spec:
  claimRef:
    apiVersion: v1
    kind: PersistentVolumeClaim
    name: rbd-pvc
    namespace: default
    resourceVersion: "2512"
    uid: 0b93de3a-3946-4909-a189-f44dba0abc76
 ...
```

> Remove the claimRef in all PV backup's

```bash=
vi pv_bkp.yaml
```

## Planned Storage Migration

### Failover

Follow the Below steps for planned migration when you want to move the workload from one cluster to another cluster

* Scale down all the application pods which are using the mirrored PVC
* Take a back up of PVC and PV object from the cluster1
  * This can be done using some backup tools like velero also
* update replicationState to secondary in VolumeReplication CR at Primary Site. When the operator sees this change, it will pass the information down to the driver via GRPC request to mark the dataSource as secondary.
* If you are manually recreating the PVC and PV on the secondary cluster. remove the claimRef section in the PV objects.
* Recreate the storageclass, PVC, and PV objects on the cluster2
  * As you are creating the static binding between PVC and PV, a new PV won't be created here, the PVC will get bind to the existing PV.
* Create the VolumeReplicationClass on the cluster2
* Create the VolumeReplications for all the PVC's for which mirroring is enabled
  * `replicationState` should be `primary` for all the PVC's on the cluster1
* Check whether the image is primary on cluster2 by looking at toolbox or VolumeReplication CR status
* Once the Image is marked as Primary create the application to use the PVC
In the case of planned migration,

#### Failback

Once the planned work is done and you want to failback, follow the above Failover steps again. the only difference, in this case, is we don't need to recreate anything we just need to change `replicationState` from `secondary` to `primary` in current cluster1.

## Storage Disaster Recovery

### Failover

In case of disaster recovery, create VolumeReplication CR at the Secondary Site. Since the connection to the Primary Site is lost, the operator automatically sends a GRPC request down to the driver to forcefully mark the dataSource as primary.

* If you are manually recreating the PVC and PV on the secondary cluster. remove the claimRef section in the PV objects.
* Recreate the storageclass, PVC, and PV objects on the cluster2
  * As you are creating the static binding between PVC and PV, a new PV won't be created here, the PVC will get bind to the existing PV.
* Create the VolumeReplicationClass and VolumeReplication CR on the cluster2
* Check whether the image is primary on cluster2 by looking at toolbox or VolumeReplication CR status
* Once the Image is marked as Primary create the application to use the PVC
In the case of planned migration,

### Failback

Once the failed cluster is recovered and you want to failback, follow the below steps

* Update the VolumeReplication CR `replicationState` state from `primary` to `secondary` in cluster1
* Scale down the applications on the cluster2
* Update the VolumeReplication CR `replicationState` state from `primary` to `secondary` in cluster2
* On the cluster1 verify that the VR status is marked as volume ready to use
* Once the volume is marked to ready to use, change the `replicationState` state from `secondary` to `primary` in cluster1
* Scale up the applications again on cluster1]]></content><author><name></name></author><summary type="html"><![CDATA[This doc assumes that you already set up the rbd mirroring between two clusters.]]></summary></entry><entry><title type="html">Troubleshooting Cephcsi</title><link href="https://mrajanna.com/troubleshooting-cephcsi/" rel="alternate" type="text/html" title="Troubleshooting Cephcsi" /><published>2021-01-25T00:00:00+01:00</published><updated>2021-01-25T00:00:00+01:00</updated><id>https://mrajanna.com/troubleshooting-cephcsi</id><content type="html" xml:base="https://mrajanna.com/troubleshooting-cephcsi/"><![CDATA[The issue in Provisioning the CephFS/RBD PVC or Mounting the CephFS/RBD PVC to
application pods can happen for Many reasons like **`Network connectivity
between csi pods and ceph`**, **`cluster health issue`**,**`Slow operations`**,
**`kubernetes issue`**, etc. Even the issue can be at the Ceph-CSI layer
itself. Hope the below-documented steps might help you to understand and Debug
the cephcsi issues.

## How to identify network issues

Get the monitor IP from the configmap you have created when starting the
cephcsi pods or from the ceph cluster.

```bash=
sh-4.4# ceph mon dump
dumped monmap epoch 1
epoch 1
fsid ba41ac93-3b55-4f32-9e06-d3d8c6ff7334
last_changed 2021-01-20T12:28:37.972517+0000
created 2021-01-20T12:28:37.972517+0000
min_mon_release 15 (octopus)
0: [v2:10.111.136.166:3300/0,v1:10.111.136.166:6789/0] mon.a
```

**Note:-** `10.111.136.166`is the Monitor IP and `6789` is the monitor port
that will be used by CephCSI to connect to the ceph cluster.

> Ceph Monitors normally listen on port 3300 for the new v2 protocol, and 6789
> for the old v1 protocol. Check if your cluster is listening on v2 or v1
> protocol.

### Check network connectivity on your kubernetes nodes

If you are seeing the issue in Provisioning the PVC then you need to check the
network connectivity from the Provisioner pods.

* CephFS

If it's **CephFS PVC** then you need to check network connectivity from the
**csi-cephfsplugin** container of the **csi-cephfsplugin-provisioner** Pods.

* RBD

If it's **RBD PVC** then you need to check network connectivity from the
**csi-rbdplugin** container of the **csi-rbdplugin-provisioner** Pods.

> Based on the kubernetes nodes count or the configuration there might be 1 or
> more provisioner pods will be running. It's good to validate you can check
> the ceph cluster from all provisioner pods.

```bash=
[üé©Ô∏é]mrajanna@localhost $]cat < /dev/tcp/10.111.136.166/6789
ceph v027ÔøΩÔøΩÔøΩ'eÔøΩÔøΩÔøΩÔøΩ'^C
[üé©Ô∏é]mrajanna@localhost $] cat < /dev/tcp/10.111.136.166/3300
ceph v2
^C
```

> If you have multiple monitors make sure you check network connectivity for
> all monitor IP's and ports which are passed to cephcsi.

> If the above commands do not return any response then it will be a network
> issue to connect to the ceph cluster.

## check ceph cluster is healhty

Sometimes unhealthy ceph cluster can contribute to the issues in Creating or
mounting the pvc. Please make sure your ceph cluster is healthy

```bash=
sh-4.4# ceph health detail
HEALTH_OK
```

## check for slow ops

Even slow ops in the ceph cluster can contribute to the issues. Please make
sure that No slow Ops are present and the ceph cluster is healthy

```bash=
sh-4.4# ceph -s
  cluster:
    id:     ba41ac93-3b55-4f32-9e06-d3d8c6ff7334
    health: HEALTH_WARN
            30 slow ops, oldest one blocked for 10624 sec, mon.a has slow ops
```

Some tips on what to check.

* Look in the monitor logs
* Look in the OSD logs
  * Check Disk Health
* Check Network Health

> How to debug different issues in the ceph cluster is documented at [**ceph
> troubleshooting**](https://access.redhat.com/documentation/en-us/red_hat_ceph_storage/4/html/troubleshooting_guide/index)

## Pool or subvolumegroup exists in the ceph cluster

* RBD

Make sure the pool you have specified in the
[storageclass.yaml](https://github.com/rook/rook/blob/master/cluster/examples/kubernetes/ceph/csi/rbd/storageclass.yaml#L34)
Exists in the ceph cluster.

Suppose the pool name is mentioned in the storageclass.yaml is replicapool. It
can be verified as below.

```bash=
sh-4.4# ceph osd lspools
1 device_health_metrics
2 replicapool
```

>**If the pool does not exists make sure you create the pool before proceeding.**

* CephFS

Make sure the filesystem and pool you have specified in the
[storageclass.yaml](https://github.com/rook/rook/blob/master/cluster/examples/kubernetes/ceph/csi/cephfs/storageclass.yaml)
Exist in the ceph cluster.

Suppose the filesystem name mentioned in the storageclass.yaml is `myfs`. It
can be verified as below.

```bash=
sh-4.4# ceph fs ls
name: myfs, metadata pool: myfs-metadata, data pools: [myfs-data0 ]
```

> In case, if you have a specified pool name make sure it is available in the
> above data pools list.

## Check cephcsi can create subvolumegroup

If the subvolumegroup is not specified in the cephcsi configmap (where you have
passed the ceph monitor information). cephcsi creates the default
subvolumegroup when name `csi`.

```bash=
sh-4.4# ceph fs subvolumegroup ls myfs
[
    {
        "name": "csi"
    }
]
```

> **This is not applicable for Rook Users.**

Couple for cephfs issue which might help  to debug cephfs network issue
[4251](https://github.com/rook/rook/issues/4251),[6183](https://github.com/rook/rook/issues/6183)
If you don't have any issues with your ceph cluster, we can start debugging the
issue from the CSI side.

## Issue in Provisioning Volume

Most of the time the issue can also exist in the cephcsi or the sidecar
containers we are using in cephcsi.

For CephFS or RBD cephcsi has included number of sidecar containers in the
provisioner pods example **`csi-attacher` `csi-resizer` `csi-provisioner`
`csi-cephfsplugin` `csi-snapshotter` `liveness-prometheus`**

> If it's a CephFS provisioner pod you will see `csi-cephfsplugin` as one of
> the container names. If its an RBD provisioner you will see `csi-rbdplugin`
> as the container name

Let us see the functionality of the sidecar containers we use in cephcsi
provisioner pods.

* **csi-provisioner**

  The external-provisioner is a sidecar container that dynamically provisions
  volumes by calling ControllerCreateVolume and ControllerDeleteVolume
  functions of CSI drivers. More details about external-provisioner can be
  found [here](https://github.com/kubernetes-csi/external-provisioner).

  If any issue exists in PVC create or Delete operation you can check the logs
  of the csi-provisioner sidecar container.

```bash=
kubectl logs po/csi-rbdplugin-provisioner-d857bfb5f-ddctl -c csi-provisioner
```

* **csi-resizer**

  The CSI external-resizer is a sidecar container that watches the Kubernetes
  API server for PersistentVolumeClaim updates and triggers
  **ControllerExpandVolume** operations against a CSI endpoint if the user
  requested more storage on the PersistentVolumeClaim object. More details
  about external-provisioner can be found
  [here](https://github.com/kubernetes-csi/external-resizer).

  If any issue exists in PVC expansion you can check the logs of the
  csi-resizer sidecar container.

```bash=
kubectl logs po/csi-rbdplugin-provisioner-d857bfb5f-ddctl -c csi-resizer
```

* **csi-snapshotter**

  The CSI external-snapshotter sidecar only watches for VolumeSnapshotContent
  create/update/delete events. It will talk to cephcsi containers to create or
  delete snapshots. More details about external-snapshotter can be found
  [here](https://github.com/kubernetes-csi/external-snapshotter).

> In Kubernetes 1.17 when the volume snapshot feature is promoted to beta. In
> Kubernetes 1.20, the feature gate is enabled by default on standard
> Kubernetes deployments and cannot be turned off.

Make sure you have installed the right snapshotter CRD version.

```bash=
kubectl get crd |grep snapshot
volumesnapshotclasses.snapshot.storage.k8s.io    2021-01-25T11:19:38Z
volumesnapshotcontents.snapshot.storage.k8s.io   2021-01-25T11:19:39Z
volumesnapshots.snapshot.storage.k8s.io          2021-01-25T11:19:40Z
```

Check above CRD's is having Right version and you are using the Correct Version
in your **snapshotclass.yaml** or **snapshot.yaml** as well. Or else
**VolumeSnapshot** and **VolumesnapshotContent** might not get created.

The snapshot controller is responsible for creating both VolumeSnapshot and
VolumesnapshotContent object. If the objects are not getting created, You may
need to check the logs of the snapshot-controller pod.

> **Rook** only installs the snapshotter sidecar container, not the
> controller. It is strongly recommended that Kubernetes distributors bundle
> and deploy the controller and CRDs as part of their Kubernetes cluster
> management process (independent of any CSI Driver).

> If your Kubernetes distribution does not bundle the snapshot controller,
> you may manually install these
> [components](https://github.com/kubernetes-csi/external-snapshotter#usage)

If any issue exists in the snapshot Create/Delete operation you can check the
logs of the csi-snapshotter sidecar container.

```bash=
kubectl logs po/csi-rbdplugin-provisioner-d857bfb5f-ddctl -c csi-snapshotter
```

> If you see error like `GRPC error: rpc error: code = Aborted desc = an operation with the given Volume ID 0001-0009-rook-ceph-0000000000000001-8d0ba728-0e17-11eb-a680-ce6eecc894de already exists`.

```go
The issue mostly exists in ceph cluster or network connectivity. If the issue is
in Provisioning the PVC Restarting the Provisioner pods help(for CephFS issue
restart `csi-cephfsplugin-provisioner-xxxxxx` CephFS Provisioner. For RBD, restart
`csi-rbdplugin-provisioner-xxxxxx` pod. If the issue is in mounting the PVC,
restarting the csi-rbdplugin-xxxxx for RBD issue and csi-cephfsplugin-xxxxx pod
for CephFS issue helps sometimes not always.
```

IF the restarting didnt help you can execute below commands from the cephfs/rbd
provisioner pods

* For RBD provisioner pod

```bash
$kubectl exec -t csi-rbdplugin-provisioner -c csi-rbdplugin -- rbd create --size 1024 pool-name/testimage101 -m=10.11.111.11:6789 --user=csi-rbd-provisioner --key="AQC+t/5fqCS2DhAAqheqlRIIMz6mjWQ4g8mUnw==" --debug_ms=20 --debug_rbd=20

$kubectl exec -t csi-rbdplugin-provisioner -c csi-rbdplugin -- rados setomapval test testkey testval -p poolname -m=10.11.111.11:6789 --user=csi-rbd-provisioner --key="xx/xxx+MSr6O5ZMarRHw=="
```

Note:- update monitor IP, csi-rbdplugin-provisioner pod name, pool-name and
key in above command

* For CephFS provisioner pod

```bash
$kubectl exec -t csi-cephfsplugin-provisioner -c csi-cephfsplugin -- ceph fs subvolume ls <fs-name> csi -m=10.11.111.11:6789 --user=csi-cephfs-provisioner --key="AQC+t/5fqCS2DhAAqheqlRIIMz6mjWQ4g8mUnw==" --debug_ms=20

$kubectl exec -t csi-cephfsplugin-provisioner -c csi-cephfsplugin -- rados setomapval test testkey testval -p myfs-metadata --namespace=csi -m=10.11.111.11:6789 --user=csi-cephfs-provisioner --key="xx/xxx+MSr6O5ZMarRHw=="
```

Note:- update monitor IP, csi-cephfsplugin-provisioner pod name,filesystem name, pool-name and
key in above command

## Issue in Mounting the pvc to application pods

When a user requests to create the application pod with PVC. It's a three-step
process

* csi-driver Registration
* Create volume attachment object
* Stage and Publish Volume.

### csi-driver registration

`csi-cephfsplugin-xxxx` or `csi-rbdplugin-xxxx` is a daemonset pod running on
all the nodes where your application gets scheduled. If the plugin pods are not
running on the node where your application is scheduled might cause the issue,
Make sure plugin pods are always running.

Each plugin pod has 2 important container one is `driver-registrar` and
`csi-rbdplugin` or `csi-cephfsplugin`. sometimes we also deploy a
`liveness-prometheus` container.

* **driver-registrar**

  The node-driver-registrar is a sidecar container that registers the CSI
  driver with Kubelet. More datils can be found
  [here](https://github.com/kubernetes-csi/node-driver-registrar)

If any issue exists in attaching the PVC to the application pod check logs from
*driver-registrar* sidecar container in plugin pod where your application pod
is scheduled.

```bash=
$ kubectl logs po/csi-rbdplugin-vh8d5 -c driver-registrar
I0120 12:28:34.231761  124018 main.go:112] Version: v2.0.1
I0120 12:28:34.233910  124018 connection.go:151] Connecting to unix:///csi/csi.sock
I0120 12:28:35.242469  124018 node_register.go:55] Starting Registration Server at: /registration/rook-ceph.rbd.csi.ceph.com-reg.sock
I0120 12:28:35.243364  124018 node_register.go:64] Registration Server started at: /registration/rook-ceph.rbd.csi.ceph.com-reg.sock
I0120 12:28:35.243673  124018 node_register.go:86] Skipping healthz server because port set to: 0
I0120 12:28:36.318482  124018 main.go:79] Received GetInfo call: &InfoRequest{}
I0120 12:28:37.455211  124018 main.go:89] Received NotifyRegistrationStatus call: &RegistrationStatus{PluginRegistered:true,Error:,}
E0121 05:19:28.658390  124018 connection.go:129] Lost connection to unix:///csi/csi.sock.
E0125 07:11:42.926133  124018 connection.go:129] Lost connection to unix:///csi/csi.sock.
```

> You should see `RegistrationStatus{PluginRegistered:true,Error:,}` response
> in the logs to confirm that plugin is registered with kubelet.

> If you see a driver not found an error in the application pod describe
> output. restarting the csi-xxxxplugin-xxx pod on the node helps sometimes.

### Check Issue in Volume attachment

Each provisioner pod also consists of a sidecar container called csi-attacher.

* **csi-attacher**

  The external-attacher is a sidecar container that attaches volumes to nodes
  by calling ControllerPublish and ControllerUnpublish functions of CSI
  drivers. It is necessary because the internal Attach/Detach controller
  running in Kubernetes controller-manager does not have any direct interfaces
  to CSI drivers. More datils can be found
  [here](https://github.com/kubernetes-csi/external-attacher)

If any issue exists in attaching the PVC to the application pod first check the
`volumettachment` object created and also log from *csi-attacher* sidecar
container in provisioner pod.

```bash=
$ kubectl get volumeattachment
NAME                                                                   ATTACHER                        PV                                         NODE       ATTACHED   AGE
csi-75903d8a902744853900d188f12137ea1cafb6c6f922ebc1c116fd58e950fc92   rook-ceph.cephfs.csi.ceph.com   pvc-5c547d2a-fdb8-4cb2-b7fe-e0f30b88d454   minikube   true       4m26s
```

```bash=
kubectl logs po/csi-rbdplugin-provisioner-d857bfb5f-ddctl -c csi-attacher
```

### How to identify stale operations

* CephFS Check for any stale mount commands on the csi-cephfsplugin-xxxx pod on
  the node where your application pod is scheduled.

You need to exec in the `csi-cephfsplugin-xxxx` pod and grep for stale `mount`
operators.

> Identify the csi-cephfsplugin-xxxx pod running on the node where your
> application is scheduled with `kubectl get po -owide` and match the node
> names.

```bash=
$ kubectl exec -it csi-cephfsplugin-tfk2g -c csi-cephfsplugin -- sh
sh-4.4# ps -ef |grep mount
root          67      60  0 11:55 pts/0    00:00:00 grep mount
sh-4.4# ps -ef |grep ceph
root           1       0  0 Jan20 ?        00:00:26 /usr/local/bin/cephcsi --nodeid=minikube --type=cephfs --endpoint=unix:///csi/csi.sock --v=0 --nodeserver=true --drivername=rook-ceph.cephfs.csi.ceph.com --pidlimit=-1 --metricsport=9091 --forcecephkernelclient=true --metricspath=/metrics --enablegrpcmetrics=true
root          69      60  0 11:55 pts/0    00:00:00 grep ceph
```

If any commands are stuck check the **`dmesg`** logs from the node, Restarting
the csi-cephfsplugin pod also might work sometime.

If you don't see any stuck command, Make sure about network connectivity and
ceph health, and slow ops.

* RBD

Check for any stale map/mkfs/mount commands on the csi-rbdplugin-xxxx pod on
the node where your application pod is scheduled.

You need to exec in the `csi-rbdplugin-xxxx` pod and grep for stale operators
like (**`rbd map`**, **`rbd unmap`**, **`mkfs`**, **`mount`** and
**`umount`**).

> Identify the csi-rbdplugin-xxxx pod running on the node where your
> application is scheduled with `kubectl get po -owide` and match the node
> names.

```bash=
$ kubectl exec -it csi-rbdplugin-vh8d5 -c csi-rbdplugin -- sh
sh-4.4# ps -ef |grep map
root     1297024 1296907  0 12:00 pts/0    00:00:00 grep map
sh-4.4# ps -ef |grep mount
root        1824       1  0 Jan19 ?        00:00:00 /usr/sbin/rpc.mountd
ceph     1041020 1040955  1 07:11 ?        00:03:43 ceph-mgr --fsid=ba41ac93-3b55-4f32-9e06-d3d8c6ff7334 --keyring=/etc/ceph/keyring-store/keyring --log-to-stderr=true --err-to-stderr=true --mon-cluster-log-to-stderr=true --log-stderr-prefix=debug  --default-log-to-file=false --default-mon-cluster-log-to-file=false --mon-host=[v2:10.111.136.166:3300,v1:10.111.136.166:6789] --mon-initial-members=a --id=a --setuser=ceph --setgroup=ceph --client-mount-uid=0 --client-mount-gid=0 --foreground --public-addr=172.17.0.6
root     1297115 1296907  0 12:00 pts/0    00:00:00 grep mount
sh-4.4# ps -ef |grep mkfs
root     1297291 1296907  0 12:00 pts/0    00:00:00 grep mkfs
sh-4.4# ps -ef |grep umount
root     1298500 1296907  0 12:01 pts/0    00:00:00 grep umount
sh-4.4# ps -ef |grep unmap
root     1298578 1296907  0 12:01 pts/0    00:00:00 grep unmap
```

If any commands are stuck check the **`dmesg`** logs from the node, Restarting
the csi-rbdplugin pod also might work sometime.

If you don't see any stuck command, Make sure about network connectivity and
ceph health, and slow ops.

### How to check dmesg logs

Checking the *dmesg* logs on the node where pvc mounting is failing or the
csi-rbdplugin container of the csi-rbdplugin-xxxx pod on that node always
helps.

```bash=
dmesg
```

* If nothing helps get the last executed command from the cephcsi pod logs and
  run it manually inside provisioner or plugin pod

```bash=
rbd ls --id=csi-rbd-node -m=10.111.136.166:6789 --key=AQDpIQhg+v83EhAAgLboWIbl+FL/nThJzoI3Fg==
```

> Need to pass the exact user ID, key and monitor IP's and port when executing
> the command.

> If you see error like `GRPC error: rpc error: code = Aborted desc = an operation with the given Volume ID 0001-0009-rook-ceph-0000000000000001-8d0ba728-0e17-11eb-a680-ce6eecc894de already exists`.

```go
The issue mostly exists in ceph cluster or network connectivity.
If the issue is in mounting the PVC, restarting the csi-rbdplugin-xxxxx
for RBD issue and csi-cephfsplugin-xxxxx pod for CephFS issue helps.
```

IF the restarting didnt help you can execute below commands from the cephfs/rbd
plugin pod where you are seeing above error message

* For RBD plugin pod

```bash
$kubectl exec -t csi-rbdplugin-xxx -c csi-rbdplugin -- rbd ls pool-name -m=10.11.111.11:6789 --user=csi-rbd-node --key="AQC+t/5fqCS2DhAAqheqlRIIMz6mjWQ4g8mUnw==" --debug_ms=20 --debug_rbd=20

$kubectl exec -t csi-rbdplugin-xxxx -c csi-rbdplugin -- rados listomapvals test -p replicapool -m=10.11.111.11:6789 --user=csi-rbd-node --key="xx/xxx+MSr6O5ZMarRHw=="
```

Note:- update monitor IP, csi-rbdplugin pod name, pool-name and
key in above command

* For CephFS plugin pod

```bash
$kubectl exec -t csi-cephfsplugin-xxx -c csi-cephfsplugin -- ceph fs subvolume ls <fs-name> csi -m=10.11.111.11:6789 --user=csi-cephfs-node --key="AQC+t/5fqCS2DhAAqheqlRIIMz6mjWQ4g8mUnw==" --debug_ms=20

$kubectl exec -t csi-cephfsplugin-xxx -c csi-cephfsplugin -- rados getomapvals test -p poolname --namespace=csi -m=10.11.111.11:6789 --user=csi-cephfs-node --key="xx/xxx+MSr6O5ZMarRHw=="
```

Note:- update monitor IP, csi-cephfsplugin pod name,filesystem name, pool-name and
key in above command]]></content><author><name></name></author><summary type="html"><![CDATA[The issue in Provisioning the CephFS/RBD PVC or Mounting the CephFS/RBD PVC to application pods can happen for Many reasons like Network connectivity between csi pods and ceph, cluster health issue,Slow operations, kubernetes issue, etc. Even the issue can be at the Ceph-CSI layer itself. Hope the below-documented steps might help you to understand and Debug the cephcsi issues.]]></summary></entry><entry><title type="html">Play with RBD Async mirroring with minikube</title><link href="https://mrajanna.com/setup-rbd-async-mirroring-with-rook/" rel="alternate" type="text/html" title="Play with RBD Async mirroring with minikube" /><published>2020-11-23T00:00:00+01:00</published><updated>2020-11-23T00:00:00+01:00</updated><id>https://mrajanna.com/setup-rbd-async-mirroring-with-rook</id><content type="html" xml:base="https://mrajanna.com/setup-rbd-async-mirroring-with-rook/"><![CDATA[This doc assumes that you already have a minikube [installed](https://minikube.sigs.k8s.io/docs/start/), minikube is configured to use `kvm2` to create a virtual machine. You can refer [kvm2](https://minikube.sigs.k8s.io/docs/drivers/kvm2/) to install the required prerequisites.

> Install [`kubectl`](https://kubernetes.io/docs/tasks/tools/install-kubectl/) on the machine where you want to create kubernetes clusters and from where you want to access kubernetes
To Play with RBD async mirroring we need to have two kubernetes/OCP clusters as it's for just let's use minikube to create 2 kubernetes clusters.

## Create Kubernetes cluster

### Create kubernetes cluster1

```bash=
$minikube start --force --memory="4096" --cpus="2" -b kubeadm --kubernetes-version="v1.19.2" --driver="kvm2" --feature-gates="BlockVolume=true,CSIBlockVolume=true,VolumeSnapshotDataSource=true,ExpandCSIVolumes=true" --profile="cluster1"
üòÑ  [cluster1] minikube v1.14.0 on Fedora 32
‚ùó  minikube skips various validations when --force is supplied; this may lead to unexpected behavior
‚ú®  Using the kvm2 driver based on user configuration
üõë  The "kvm2" driver should not be used with root privileges.
üí°  If you are running minikube within a VM, consider using --driver=none:
üìò    https://minikube.sigs.k8s.io/docs/reference/drivers/none/
üëç  Starting control plane node cluster1 in cluster cluster1
üî•  Creating kvm2 VM (CPUs=2, Memory=4096MB, Disk=20000MB) ...
üê≥  Preparing Kubernetes v1.19.2 on Docker 19.03.12 ...
üîé  Verifying Kubernetes components...
üåü  Enabled addons: storage-provisioner, default-storageclass
üèÑ  Done! kubectl is now configured to use "cluster1" by default
```

>Note:- `--profile` is the name of the minikube VM being used. This can be set to allow having multiple instances of minikube independently. (default "minikube, so we are going to set it to `cluster1` as we are planning to create 2 minikube clusters

### Create kubernetes cluster2

We are going to use the same `minikube start` command to create kubernetes cluster but we are going to change the `--profile` name.

```bash=
$minikube start --force --memory="4096" --cpus="2" -b kubeadm --kubernetes-version="v1.19.2" --driver="kvm2" --feature-gates="BlockVolume=true,CSIBlockVolume=true,VolumeSnapshotDataSource=true,ExpandCSIVolumes=true" --profile="cluster2"
üòÑ  [cluster2] minikube v1.14.0 on Fedora 32
‚ùó  minikube skips various validations when --force is supplied; this may lead to unexpected behavior
‚ú®  Using the kvm2 driver based on user configuration
üõë  The "kvm2" driver should not be used with root privileges.
üí°  If you are running minikube within a VM, consider using --driver=none:
üìò    https://minikube.sigs.k8s.io/docs/reference/drivers/none/
üëç  Starting control plane node cluster2 in cluster cluster2
üî•  Creating kvm2 VM (CPUs=2, Memory=4096MB, Disk=20000MB) ...
üê≥  Preparing Kubernetes v1.19.2 on Docker 19.03.12 ...
üîé  Verifying Kubernetes components...
üåü  Enabled addons: storage-provisioner, default-storageclass
üèÑ  Done! kubectl is now configured to use "cluster2" by default
```

### Verify kubernetes clusters

As we have created the 2 kubernetes clusters, Let's verify it. Am going to use `--context` with kubectl to talk to different clusters

```bash=
[root@dhcp53-215 ~]$ kubectl get nodes --context=cluster1
NAME       STATUS   ROLES    AGE     VERSION
cluster1   Ready    master   8m14s   v1.19.2
[root@dhcp53-215 $ kubectl get nodes --context=cluster2
NAME       STATUS   ROLES    AGE     VERSION
cluster2   Ready    master   3m19s   v1.19.2
```

> The value  for the `--context` will be the name we passed in `--profile` when creating kubernetes clusters

As we got the kubernetes cluster, we need to add a disk to minikube VM as Rook needs a raw device to create ceph OSD and also we need to create a directory that is required to store monitor details.

### Add Device to minikube VM's

```bash=
minikube ssh "sudo mkdir -p /mnt/vda1/var/lib/rook;sudo ln -s /mnt/vda1/var/lib/rook /var/lib/rook" --profile="cluster1"
```

> Repeat the same step for cluster2 minikube VM also just change the `--profile` value to `cluster2`

Now let's add a device to minikube vm's

```bash=
$sudo -S qemu-img create -f raw /var/lib/libvirt/images/minikube-box2-vm-disk-"cluster1"-50G 50G
$virsh -c qemu:///system attach-disk "cluster1" --source /var/lib/libvirt/images/minikube-box2-vm-disk-"cluster1"-50G --target vdb --cache none
$virsh -c qemu:///system reboot --domain "cluster1"
```

### Verify that thedevice is present in the minikube VM

```bash=
$minikube ssh --profile=cluster1
                         _             _
            _         _ ( )           ( )
  ___ ___  (_)  ___  (_)| |/')  _   _ | |_      __
/' _ ` _ `\| |/' _ `\| || , <  ( ) ( )| '_`\  /'__`\
| ( ) ( ) || || ( ) || || |\`\ | (_) || |_) )(  ___/
(_) (_) (_)(_)(_) (_)(_)(_) (_)`\___/'(_,__/'`\____)

$ ls /dev/vd
vda   vda1  vdb
$ ls /dev/vd
vda   vda1  vdb
$ ls /dev/vdb
/dev/vdb
$ exit
```

> If the device is not visible inside the minikube VM, you need to start the minikube with the same command `minikube start` we used above to create the kubernetes cluster.

> Repeat the same steps to add a  device to the other kubernetes cluster  `cluster2`, don't forget to change `--profile` value to `cluster2`.


## Install Rook

Now we have the basic infrastructure to create a Ceph cluster. let's install Rook on both kubernetes clusters

```bash=
$git clone git@github.com:rook/rook.git
$ cd rook
```

### Create Required RBAC and CRD
```bash=
$kubectl create -f cluster/examples/kubernetes/ceph/common.yaml --context=cluster1
$kubectl create -f cluster/examples/kubernetes/ceph/crds.yaml --context=cluster1
```

### Install Rook operator

```bash=
$kubectl create -f cluster/examples/kubernetes/ceph/operator.yaml --context=cluster1
configmap/rook-ceph-operator-config created
deployment.apps/rook-ceph-operator created
```

As you already know that for RBD async mirroring RBD daemon need to talk to each other so am going to use hostNetworking for now to create a Ceph cluster

### Create ceph cluster

```bash=
$cat <<EOF | kubectl --context=cluster1 apply -f -
kind: ConfigMap
apiVersion: v1
metadata:
  name: rook-config-override
  namespace: rook-ceph
data:
  config: |
    [global]
    osd_pool_default_size = 1
    mon_warn_on_pool_no_redundancy = false
---
apiVersion: ceph.rook.io/v1
kind: CephCluster
metadata:
  name: my-cluster
  namespace: rook-ceph
spec:
  dataDirHostPath: /var/lib/rook
  cephVersion:
    image: ceph/ceph:v15
    allowUnsupported: true
  mon:
    count: 1
    allowMultiplePerNode: true
  dashboard:
    enabled: true
  crashCollector:
    disable: true
  storage:
    useAllNodes: true
    useAllDevices: true
  network:
    provider: host
  healthCheck:
    daemonHealth:
      mon:
        interval: 45s
        timeout: 600s
EOF
```
### Create toolbox pod

```bash=
$kubectl create -f cluster/examples/kubernetes/ceph/toolbox.yaml --context=cluster1
deployment.apps/rook-ceph-tools created
```

### Verify all pods are in running state

```bash=
$kubectl get po --context=cluster1 -nrook-ceph
NAME                                            READY   STATUS      RESTARTS   AGE
csi-cephfsplugin-gzgxm                          3/3     Running     0          21m
csi-cephfsplugin-provisioner-6cfdb89f9b-5bqbf   6/6     Running     0          21m
csi-cephfsplugin-provisioner-6cfdb89f9b-d2bp6   0/6     Pending     0          21m
csi-rbdplugin-ffpsh                             3/3     Running     0          21m
csi-rbdplugin-provisioner-785d8b7967-wxfp7      0/7     Pending     0          21m
csi-rbdplugin-provisioner-785d8b7967-zrv5x      7/7     Running     0          21m
rook-ceph-mgr-a-7c74849566-ghmqp                1/1     Running     0          24m
rook-ceph-mon-a-5bb9f8787d-v5n4m                1/1     Running     0          24m
rook-ceph-operator-d758658ff-4ldqs              1/1     Running     0          33m
rook-ceph-osd-0-56d989d9cc-6rsxl                1/1     Running     0          24m
rook-ceph-osd-prepare-cluster1-hpxfp            0/1     Completed   0          24m
rook-ceph-tools-78cdfd976c-jj98r                1/1     Running     0          3m30s
rook-discover-p4dfs                             1/1     Running     0          32m
```

> Repeat the same steps we did to install rook. Now install Rook on cluster2. Don't forget to change the `--context` value


Once we have Rook installed on both the clusters, we can create pools, configure mirroring and try out rbd async replication

### Create RBD Block Pool

Rook allows the creation and customization of storage pools through the custom resource definitions (CRDs), Let's create a Pool with Name `replicapool` with mirroring Enabled

```bash=
$cat <<EOF | kubectl --context=cluster1 apply -f -
apiVersion: ceph.rook.io/v1
kind: CephBlockPool
metadata:
  name: replicapool
  namespace: rook-ceph
spec:
  replicated:
    size: 1
  mirroring:
    enabled: true
    mode: image
    # schedule(s) of snapshot
    snapshotSchedules:
      - interval: 24h # daily snapshots
        startTime: 14:00:00-05:00
EOF
```

> Repeat the same steps on cluster2. Don't forget to change the `--context` value

Once mirroring is enabled, Rook will by default create its own bootstrap peer token so that it can be used by another cluster. The bootstrap peer token can be found in a Kubernetes Secret. The name of the Secret is present in the Status field of the CephBlockPool CR:

```bash=
$kubectl get cephblockpools.ceph.rook.io replicapool --context=cluster1 -nrook-ceph -o jsonpath='{.status.info}'
{"rbdMirrorBootstrapPeerSecretName":"pool-peer-token-replicapool"}
```

This secret can then be fetched like so:

```bash=
$kubectl get secret -n rook-ceph pool-peer-token-replicapool --context=cluster1 -o jsonpath='{.data.token}'|base64 -d
eyJmc2lkIjoiZjA1YzEwMGYtNjJkYS00YzU4LWI4OTktMzQyZTM0ZDg4MDNkIiwiY2xpZW50X2lkIjoicmJkLW1pcnJvci1wZWVyIiwia2V5IjoiQVFBNFU3dGZndDl6T3hBQUZXbEorYUFNUFo5MXpqNlRwUE84V3c9PSIsIm1vbl9ob3N0IjoiW3YyOjE5Mi4xNjguMzkuODQ6MzMwMCx2MToxOTIuMTY4LjM5Ljg0OjY3ODldIn0=
```

### Create RBD mirror daemon CRD

Rook allows the creation and updating of the rbd-mirror daemon(s) through the custom resource definitions (CRDs). RBD images can be asynchronously mirrored between two Ceph clusters.

#### Configure RBD mirroring on cluster1

Let's configure the RBD mirror daemon on the cluster1, To do that we need to get the pool secret and site_name from the cluster2

##### Get site_name from the cluster2

```bash=
$kubectl get cephblockpools.ceph.rook.io replicapool --context=cluster2 -nrook-ceph -o jsonpath='{.status.mirroringInfo.summary.summary.site_name}'
e1877a97-6607-4baa-b477-64b0c61f268f-rook-ceph
```

##### Get token from the cluster2

```bash=
$kubectl get secret -n rook-ceph pool-peer-token-replicapool --context=cluster2 -o jsonpath='{.data.token}'|base64 -d
eyJmc2lkIjoiZTE4NzdhOTctNjYwNy00YmFhLWI0NzctNjRiMGM2MWYyNjhmIiwiY2xpZW50X2lkIjoicmJkLW1pcnJvci1wZWVyIiwia2V5IjoiQVFCZ1U3dGZydUh5THhBQVBMRE9EajJRMEcybjRCd2tDbmxLQUE9PSIsIm1vbl9ob3N0IjoiW3YyOjE5Mi4xNjguMzkuMTYwOjMzMDAsdjE6MTkyLjE2OC4zOS4xNjA6Njc4OV0ifQ==
```

When the peer token is available, you need to create a Kubernetes Secret. Our `e1877a97-6607-4baa-b477-64b0c61f268f-rook-ceph` will have to be created manually, like so:

##### Create secret for RBD mirroring on cluster1

```bash=
$kubectl -n rook-ceph create secret generic --context=cluster1 "e1877a97-6607-4baa-b477-64b0c61f268f-rook-ceph" \
--from-literal=token=eyJmc2lkIjoiZTE4NzdhOTctNjYwNy00YmFhLWI0NzctNjRiMGM2MWYyNjhmIiwiY2xpZW50X2lkIjoicmJkLW1pcnJvci1wZWVyIiwia2V5IjoiQVFCZ1U3dGZydUh5THhBQVBMRE9EajJRMEcybjRCd2tDbmxLQUE9PSIsIm1vbl9ob3N0IjoiW3YyOjE5Mi4xNjguMzkuMTYwOjMzMDAsdjE6MTkyLjE2OC4zOS4xNjA6Njc4OV0ifQ== \
--from-literal=pool=replicapool
secret/e1877a97-6607-4baa-b477-64b0c61f268f-rook-ceph created
```

> Pass the correct pool name and token retrieved from cluster2

Rook will read both token and pool keys of the Data content of the Secret. Rook also accepts the destination key, which specifies the mirroring direction. It defaults to rx-tx for bidirectional mirroring, but can also be set to rx-only for unidirectional mirroring.

##### Create mirroring CRD on cluster1

You can now inject the rbdmirror CR:

```bash=
$cat <<EOF | kubectl --context=cluster1 apply -f -
apiVersion: ceph.rook.io/v1
kind: CephRBDMirror
metadata:
  name: my-rbd-mirror
  namespace: rook-ceph
spec:
  count: 1
  peers:
    secretNames:
      - "e1877a97-6607-4baa-b477-64b0c61f268f-rook-ceph"
EOF
cephrbdmirror.ceph.rook.io/my-rbd-mirror created
```
> Update the secretName in Mirror CRD with the one you created above.

##### Validate mirroring health on cluster1

Once we create mirror CRD we need to validate two things.
1. Mirroring Pod in  rook-ceph namespace
2. Blockpool mirroring info

###### Check mirroring pod status

```bash=
$kubectl get po -nrook-ceph --context=cluster1
NAME                                            READY   STATUS      RESTARTS   AGE
csi-cephfsplugin-8jj5b                          3/3     Running     0          28m
csi-cephfsplugin-provisioner-7dc78747bf-426cp   6/6     Running     0          28m
csi-rbdplugin-64w97                             3/3     Running     0          28m
csi-rbdplugin-provisioner-54d48757b4-4hzml      6/6     Running     0          28m
rook-ceph-mgr-a-f59fd65f4-bwnlb                 1/1     Running     0          27m
rook-ceph-mon-a-7f754c4768-rwz29                1/1     Running     0          28m
rook-ceph-operator-559b6fcf59-77tct             1/1     Running     0          29m
rook-ceph-osd-0-6774f57f64-qmjqx                1/1     Running     0          27m
rook-ceph-osd-prepare-cluster1-n8vqb            0/1     Completed   0          27m
rook-ceph-rbd-mirror-a-6cbdcd6cc9-cfsj2         1/1     Running     0          6m33s
```

###### Check blockpool mirroring status

```bash=
$kubectl get cephblockpools.ceph.rook.io replicapool --context=cluster1 -nrook-ceph -o jsonpath='{.status.mirroringStatus.summary.summary}'
{"daemon_health":"OK","health":"OK","image_health":"OK","states":{}}
```

> You need to wait till all fields in `summary` become `OK`

Wow!! Everything seems fine let's configure the mirroring daemon on cluster2

#### Configure RBD mirroring on cluster2

Configure the RBD mirror daemon on the cluster2, To do that we need to get the pool secret and site_name from the cluster1

##### Get site_name from the cluster1

```bash=
$kubectl get cephblockpools.ceph.rook.io replicapool --context=cluster1 -nrook-ceph -o jsonpath='{.status.mirroringInfo.summary.summary.site_name}'
f05c100f-62da-4c58-b899-342e34d8803d-rook-ceph
```

##### Get token from the cluster1

```bash=
$kubectl get secret -n rook-ceph pool-peer-token-replicapool --context=cluster1 -o jsonpath='{.data.token}'|base64 -d
eyJmc2lkIjoiZjA1YzEwMGYtNjJkYS00YzU4LWI4OTktMzQyZTM0ZDg4MDNkIiwiY2xpZW50X2lkIjoicmJkLW1pcnJvci1wZWVyIiwia2V5IjoiQVFBNFU3dGZndDl6T3hBQUZXbEorYUFNUFo5MXpqNlRwUE84V3c9PSIsIm1vbl9ob3N0IjoiW3YyOjE5Mi4xNjguMzkuODQ6MzMwMCx2MToxOTIuMTY4LjM5Ljg0OjY3ODldIn0=
```

When the peer token is available, you need to create a Kubernetes Secret. Our `f05c100f-62da-4c58-b899-342e34d8803d-rook-ceph` will have to be created manually, like so:

##### Create secret for RBD mirroring on cluster2

```bash=
$kubectl -n rook-ceph create secret generic --context=cluster2 "f05c100f-62da-4c58-b899-342e34d8803d-rook-ceph" \
--from-literal=token=eyJmc2lkIjoiZjA1YzEwMGYtNjJkYS00YzU4LWI4OTktMzQyZTM0ZDg4MDNkIiwiY2xpZW50X2lkIjoicmJkLW1pcnJvci1wZWVyIiwia2V5IjoiQVFBNFU3dGZndDl6T3hBQUZXbEorYUFNUFo5MXpqNlRwUE84V3c9PSIsIm1vbl9ob3N0IjoiW3YyOjE5Mi4xNjguMzkuODQ6MzMwMCx2MToxOTIuMTY4LjM5Ljg0OjY3ODldIn0= \
--from-literal=pool=replicapool
secret/f05c100f-62da-4c58-b899-342e34d8803d-rook-ceph created
```

> Pass the correct pool name and token retrieved from cluster1

##### Create mirroring CRD on cluster2

You can now inject the rbdmirror CR:

```bash=
$cat <<EOF | kubectl --context=cluster2 apply -f -
apiVersion: ceph.rook.io/v1
kind: CephRBDMirror
metadata:
  name: my-rbd-mirror
  namespace: rook-ceph
spec:
  count: 1
  peers:
    secretNames:
      - "f05c100f-62da-4c58-b899-342e34d8803d-rook-ceph"
EOF
cephrbdmirror.ceph.rook.io/my-rbd-mirror created
```
> Update the secretName in CRD with the one you created.

##### Validate mirroring health on cluster1

Once we create mirror CRD we need to validate two things.
1. Mirroring Pod in  rook-ceph namespace
2. Blockpool mirroring info

###### Check mirroring pod status

```bash=
$kubectl get po -nrook-ceph --context=cluster2
NAME                                            READY   STATUS      RESTARTS   AGE
csi-cephfsplugin-pkrhb                          3/3     Running     0          38m
csi-cephfsplugin-provisioner-7dc78747bf-gr99q   6/6     Running     0          38m
csi-rbdplugin-provisioner-54d48757b4-4kpw5      6/6     Running     0          38m
csi-rbdplugin-xwz6j                             3/3     Running     0          38m
rook-ceph-mgr-a-565774f8fb-b66xs                1/1     Running     0          38m
rook-ceph-mon-a-88b5d6d95-vkgsb                 1/1     Running     0          38m
rook-ceph-operator-559b6fcf59-mzbpb             1/1     Running     0          39m
rook-ceph-osd-0-557d5f5948-9qrvq                1/1     Running     0          37m
rook-ceph-osd-prepare-cluster2-j9ccs            0/1     Completed   0          38m
rook-ceph-rbd-mirror-a-7c6d49f67c-b2vd2         1/1     Running     0          83s
```

###### Check blockpool mirroring status

```bash=
$kubectl get cephblockpools.ceph.rook.io replicapool --context=cluster2 -nrook-ceph -o jsonpath='{.status.mirroringStatus.summary.summary}'
{"daemon_health":"OK","health":"OK","image_health":"OK","states":{}}
```

> You need to wait till all fields in `summary` become `OK`

Now the installation is complete, Let's see we can create rbd images and able to mirror it to the `secondary` cluster.

### Provision storage

Create a `storageclass` and `PVC`

#### Create storageclass

```bash=
$cat <<EOF | kubectl --context=cluster1 apply -f -
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
   name: rook-ceph-block
provisioner: rook-ceph.rbd.csi.ceph.com
parameters:
    clusterID: rook-ceph
    pool: replicapool
    imageFormat: "2"
    imageFeatures: layering
    csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
    csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
    csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
    csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph
    csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
    csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph
    csi.storage.k8s.io/fstype: ext4
reclaimPolicy: Retain
EOF
storageclass.storage.k8s.io/rook-ceph-block created
```

> Create a storageclass with `Retain` `reclaimPolicy` so that we can use static binding for
DR.

### Create PersistentVolumeClaim on cluster1

```bash=
$cat <<EOF | kubectl --context=cluster1 apply -f -
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: rbd-pvc
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
  storageClassName: rook-ceph-block
EOF
```

Check PVC is in bound state

```bash=
$kubectl get pvc --context=cluster1
NAME      STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS      AGE
rbd-pvc   Bound    pvc-57cfca36-cb76-4d79-b258-a107bc5c4d15   1Gi        RWO            rook-ceph-block   44s
```

Get the RBD image name created for this PVC

```bash=
$kubectl get pv --context=cluster1 $(kubectl get pvc rbd-pvc  --context=cluster1 -o jsonpath='{.spec.volumeName}') -o jsonpath='{.spec.csi.volumeAttributes.imageName}'
csi-vol-1b82d349-2d5a-11eb-b8d5-0242ac110004
```

Run commands on toolbox pod on cluster1 to enable mirroring

```bash=
$kubectl get po --context=cluster1 -nrook-ceph -l  app=rook-ceph-tools
NAME                               READY   STATUS    RESTARTS   AGE
rook-ceph-tools-78cdfd976c-jrhjm   1/1     Running   0          3m53s
```

```bash=
$kubectl exec --context=cluster1 -nrook-ceph rook-ceph-tools-78cdfd976c-jrhjm -- rbd info csi-vol-1b82d349-2d5a-11eb-b8d5-0242ac110004 --pool=replicapool
rbd image 'csi-vol-1b82d349-2d5a-11eb-b8d5-0242ac110004':
	size 1 GiB in 256 objects
	order 22 (4 MiB objects)
	snapshot_count: 0
	id: 13a672b41f5f
	block_name_prefix: rbd_data.13a672b41f5f
	format: 2
	features: layering
	op_features:
	flags:
	create_timestamp: Mon Nov 23 07:04:21 2020
	access_timestamp: Mon Nov 23 07:04:21 2020
	modify_timestamp: Mon Nov 23 07:04:21 2020
```

```bash=
kubectl exec --context=cluster1 -nrook-ceph rook-ceph-tools-78cdfd976c-jrhjm -- rbd mirror image enable csi-vol-1b82d349-2d5a-11eb-b8d5-0242ac110004 snapshot --pool=replicapool
Mirroring enabled
```

Verify that mirroring enabled on the image

```bash=
$kubectl exec --context=cluster1 -nrook-ceph rook-ceph-tools-78cdfd976c-jrhjm -- rbd info csi-vol-1b82d349-2d5a-11eb-b8d5-0242ac110004 --pool=replicapool
rbd image 'csi-vol-1b82d349-2d5a-11eb-b8d5-0242ac110004':
	size 1 GiB in 256 objects
	order 22 (4 MiB objects)
	snapshot_count: 1
	id: 13a672b41f5f
	block_name_prefix: rbd_data.13a672b41f5f
	format: 2
	features: layering
	op_features:
	flags:
	create_timestamp: Mon Nov 23 07:04:21 2020
	access_timestamp: Mon Nov 23 07:04:21 2020
	modify_timestamp: Mon Nov 23 07:04:21 2020
	mirroring state: enabled
	mirroring mode: snapshot
	mirroring global id: 7da10ec2-aa67-4b05-86f0-487b4fa9fdbb
	mirroring primary: true
```

#### Verify that image is mirrored on the secondary cluster

Run commands on toolbox pod on cluster1 to enable mirroring

```bash=
$kubectl get po --context=cluster2 -nrook-ceph -l  app=rook-ceph-tools
NAME                               READY   STATUS    RESTARTS   AGE
rook-ceph-tools-78cdfd976c-2566m   1/1     Running   0          3m53s
```

```bash=
$kubectl exec --context=cluster2 -nrook-ceph rook-ceph-tools-78cdfd976c-2566m -- rbd info csi-vol-1b82d349-2d5a-11eb-b8d5-0242ac110004 --pool=replicapool
rbd image 'csi-vol-1b82d349-2d5a-11eb-b8d5-0242ac110004':
	size 1 GiB in 256 objects
	order 22 (4 MiB objects)
	snapshot_count: 1
	id: 12a55edc8361
	block_name_prefix: rbd_data.12a55edc8361
	format: 2
	features: layering, non-primary
	op_features:
	flags:
	create_timestamp: Mon Nov 23 07:12:33 2020
	access_timestamp: Mon Nov 23 07:12:33 2020
	modify_timestamp: Mon Nov 23 07:12:33 2020
	mirroring state: enabled
	mirroring mode: snapshot
	mirroring global id: 7da10ec2-aa67-4b05-86f0-487b4fa9fdbb
	mirroring primary: false
```]]></content><author><name></name></author><summary type="html"><![CDATA[This doc assumes that you already have a minikube installed, minikube is configured to use kvm2 to create a virtual machine. You can refer kvm2 to install the required prerequisites.]]></summary></entry><entry><title type="html">Set up external ceph cluster with Rook</title><link href="https://mrajanna.com/setup-external-ceph-with-rook/" rel="alternate" type="text/html" title="Set up external ceph cluster with Rook" /><published>2020-10-30T00:00:00+01:00</published><updated>2020-10-30T00:00:00+01:00</updated><id>https://mrajanna.com/setup-external-ceph-with-rook</id><content type="html" xml:base="https://mrajanna.com/setup-external-ceph-with-rook/"><![CDATA[This blog will help you out understand the various configuration we need to do
to manage the external ceph cluster with Rook.

## Checkout released Rook branch

```bash=
[üé©Ô∏é]mrajanna@localhost $]git clone https://github.com/rook/rook
[üé©Ô∏é]mrajanna@localhost $]git checkout v1.4.6
[üé©Ô∏é]mrajanna@localhost $]cd rook/cluster/examples/kubernetes/ceph
```

Let's step into the ceph directory as we are more intrested in configuring the
Rook for ceph

## Check network connectivity on your kubernetes nodes

```bash=
[üé©Ô∏é]mrajanna@localhost $]cat < /dev/tcp/192.168.39.101/6789
ceph v027ÔøΩÔøΩÔøΩ'eÔøΩÔøΩÔøΩÔøΩ'^C
[üé©Ô∏é]mrajanna@localhost $] cat < /dev/tcp/192.168.39.101/3300
ceph v2
^C
```

> Press ctrl+c as the command didn't return.

## Create Required RBAC for Rook

```bash=
[üé©Ô∏é]mrajanna@localhost $]kubectl create -f common.yaml
```

## Create operator deployment

```bash=
[üé©Ô∏é]mrajanna@localhost $]kubectl create -f operator.yaml
```

> Verify Rook operator is running

 ```bash=
[üé©Ô∏é]mrajanna@localhost $]kuberc get po
NAME                                READY   STATUS    RESTARTS   AGE
rook-ceph-operator-86756d44-vdr8b   1/1     Running   0          3m20s
rook-discover-sfjrf                 1/1     Running   0          2m47s
```

## Importing external ceph cluster

### Create RBAC for external ceph cluster

> If Rook is not managing any existing cluster in the `rook-ceph` namespace do:
> kubectl create -f common.yaml
> kubectl create -f operator.yaml
> kubectl create -f cluster-external.yaml (you need to change the namespace to `rook-ceph`)

> If there is already a cluster managed by Rook in `rook-ceph` then do:
> kubectl create -f common-external.yaml
> kubectl create -f cluster-external-management.yaml

In my case Rook is not managing any ceph cluster in `rook-ceph` namespace i
will create both `common-external.yaml` and `cluster-external-management.yaml`

```bash=
[üé©Ô∏é]mrajanna@localhost $]kubectl create -f common-external.yaml
```

### Import external ceph cluster

Export few pieces of information required for importing external ceph cluster

```bash=
export NAMESPACE=rook-ceph-external ---> Namespace where we are planning use of external ceph cluster
export ROOK_EXTERNAL_FSID=db747e90-2ede-4867-9aee-ba233aa1db55 ---> Run `ceph fsid` on your ceph cluster to get this
export ROOK_EXTERNAL_ADMIN_SECRET=AQA2cSJfOMblMRAAeHroW3THSZukFGtpkIhZ1w== ---> Run `ceph auth get-key client.admin`
on external ceph cluster
export ROOK_EXTERNAL_CEPH_MON_DATA=mon.ceph-node1=192.168.39.101:6789 ---> Run `ceph mon dump` to get the list of monitors(passing one Monitor IP should be enough)
```

>Make sure you pass correct monitor name, Provided an example below
> `mon.ceph-node1` is the monitor name for `192.168.39.101:6789`

```bash=
[üé©Ô∏é]mrajanna@localhost $]ceph mon dump
dumped monmap epoch 1
epoch 1
fsid fbafbb58-51d3-4f44-bedd-b3b728bc5766
last_changed 2020-10-27 09:48:52.247843
created 2020-10-27 09:48:52.247843
min_mon_release 14 (nautilus)
0: [v2:192.168.39.101:3300/0,v1:192.168.39.101:6789/0] mon.ceph-node1
1: [v2:192.168.39.102:3300/0,v1:192.168.39.102:6789/0] mon.ceph-node2
2: [v2:192.168.39.103:3300/0,v1:192.168.39.103:6789/0] mon.ceph-node3

```

```bash=
[üé©Ô∏é]mrajanna@localhost $] bash import-external-cluster.sh
```

### Create ceph cluster CRD

As we have created  secrets required for the external ceph cluster, Let's create
ceph cluster CRD.

```yaml
[üé©Ô∏é]mrajanna@localhost $] cat cluster-external-management.yaml
apiVersion: ceph.rook.io/v1
kind: CephCluster
metadata:
  name: rook-ceph-external
  namespace: rook-ceph-external
spec:
  external:
    enable: true -> set this to false if you want Rook to manage your external ceph cluster
  dataDirHostPath: /var/lib/rook
  # providing an image is required, if you want to create other CRs (rgw, mds, nfs)
  # In latest Rook release we dont need to provide cephVersion we can skip this one.
  cephVersion:
    image: ceph/ceph:v14.2.12 # Should match external cluster version
```

```bash=
[üé©Ô∏é]mrajanna@localhost $]kubectl create -f cluster-external-management.yaml
```

Let us check the status of the externa ceph cluster

```bash=
[üé©Ô∏é]mrajanna@localhost $]kubectl get cephcluster -nrook-ceph-external
NAME                 DATADIRHOSTPATH   MONCOUNT   AGE   PHASE        MESSAGE                 HEALTH
rook-ceph-external   /var/lib/rook                16s   Connecting   Cluster is connecting
```

> If the status is in the `Connecting` Phase for a longtime please check Rook
> operator pod which will be running in `rook-ceph` namespace.

If you don't see any useful logs in the operator pod, increase the log level of
the Rook operator deployment.

```bash=
[üé©Ô∏é]mrajanna@localhost $]kubectl edit deployment rook-ceph-operator -nrook-ceph
```

> Change `ROOK_LOG_LEVEL` from `INFO` to `DEBUG`

```yaml
- name: ROOK_LOG_LEVEL
  value: DEBUG
```

And watch for the operator logs again of any issue

> 2020-10-30 12:25:40.271439 E | cephclient: ceph username is empty
2020-10-30 12:25:40.275572 D | op-config: CephCluster "rook-ceph-external" status: "Failure". "Failed to configure external ceph cluster"

If you see an error like `ceph username` we need to remove the unwanted entry in
the secret created from the `import-external-cluster.sh`

```bash=
[üé©Ô∏é]mrajanna@localhost $]kubectl edit secret rook-ceph-mon -nrook-ceph-external
```

```yaml
apiVersion: v1
data:
  admin-secret: QVFBMmNTSmZPTWJsTVJBQWVIcm9XM1RIU1p1a0ZHdHBrSWhaMXc9PQ==
  ceph-secret: ""
  ceph-username: ""
  cluster-name: cm9vay1jZXBoLWV4dGVybmFs
  fsid: ZGI3NDdlOTAtMmVkZS00ODY3LTlhZWUtYmEyMzNhYTFkYjU1
  mon-secret: bW9uLXNlY3JldA==
kind: Secret
```

As the secret contains empty `ceph-username` and `ceph-secret` the ceph cluster
is not getting connected just remove those 2 entries from the secret and save it.

Start Watching for the operator pod log again, to see if there are any other
issues

We have got one more issue in Rook operator related to updating secret failure

> 2020-10-30 12:32:55.493617 E | ceph-cluster-controller: failed to reconcile.
> failed to reconcile cluster "rook-ceph-external": failed to configure
> external ceph cluster: failed to create csi kubernetes secrets: failed to
> create kubernetes csi secret: failed to create kubernetes secret
> map["userID":"csi-rbd-provisioner"
> "userKey":"AQAMBphfoeniNhAAtE7ZO00ZPBiJLwHU1hZnAw=="] for cluster
> "rook-ceph-external": failed to update secret for rook-csi-rbd-provisioner:
> Secret "rook-csi-rbd-provisioner" is invalid: type: Invalid value:
> "kubernetes.io/rook": field is immutable

To fix this issue lets delete all the secrets created by
`import-external-cluster.sh` script and let `Rook` create required secrets

```bash=
[üé©Ô∏é]mrajanna@localhost $]kubectl delete secret rook-csi-cephfs-node rook-csi-cephfs-provisioner rook-csi-rbd-node rook-csi-rbd-provisioner -nrook-ceph-external
secret "rook-csi-cephfs-node" deleted
secret "rook-csi-cephfs-provisioner" deleted
secret "rook-csi-rbd-node" deleted
secret "rook-csi-rbd-provisioner" deleted
```

Once we delete the secrets, let's restart the `Rook` operator pod

```bash=
[üé©Ô∏é]mrajanna@localhost $]kubectl delete po/rook-ceph-operator-675fdbc9d9-g6mjm -nrook-ceph
```

Start Watching for the operator pod log again, to see if there are any other
issues

Meanwhile, start checking the `cephclusters` status

```bash=
[üé©Ô∏é]mrajanna@localhost $]kubectl get cephclusters.ceph.rook.io -nrook-ceph-external
NAME                 DATADIRHOSTPATH   MONCOUNT   AGE   PHASE       MESSAGE                          HEALTH
rook-ceph-external   /var/lib/rook                32m   Connected   Cluster connected successfully   HEALTH_OK
```

Wow!!! Now Rook is connected to the external ceph cluster.]]></content><author><name></name></author><summary type="html"><![CDATA[This blog will help you out understand the various configuration we need to do to manage the external ceph cluster with Rook.]]></summary></entry><entry><title type="html">Track PV to RADOS omap data mapping stored by cephcsi</title><link href="https://mrajanna.com/tracking-pv-rados-omap-in-cephcsi/" rel="alternate" type="text/html" title="Track PV to RADOS omap data mapping stored by cephcsi" /><published>2020-10-22T00:00:00+02:00</published><updated>2020-10-22T00:00:00+02:00</updated><id>https://mrajanna.com/tracking-pv-rados-omap-in-cephcsi</id><content type="html" xml:base="https://mrajanna.com/tracking-pv-rados-omap-in-cephcsi/"><![CDATA[This blog will help you to understand how to track the internal rados omap data
stored by cephcsi for cephfs and rbd pvc.

Note: This blog assumes that you have rook cluster up and running and few
CephFS and RBD PVC's are created.

```bash=
[üé©Ô∏é]mrajanna@ceph $]kubectl get pvc
NAME         STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS      AGE
cephfs-pvc   Bound    pvc-88919d42-ecdf-4737-a805-065eacdfd34f   1Gi        RWO            rook-cephfs       68s
rbd-pvc      Bound    pvc-4a4c2fa3-086b-49fc-a1ef-fd8b0768f4b1   1Gi        RWO            rook-ceph-block   79s
```

In the above PVC list we have one RBD and one CephFS PVC, let us first track the
rados omap for RBD

## Track RBD omap data

To get the omap details we need to know the rbd pool in which cephcsi
stores the omap data. let's first get the pool name which is stored in PV CSI
spec

```bash=
[üé©Ô∏é]mrajanna@ceph $]kubectl get pv pvc-4a4c2fa3-086b-49fc-a1ef-fd8b0768f4b1 -o jsonpath='{.spec.csi}'
{
	"controllerExpandSecretRef": {
		"name": "rook-csi-rbd-provisioner",
		"namespace": "rook-ceph"
	},
	"driver": "rook-ceph.rbd.csi.ceph.com",
	"fsType": "ext4",
	"nodeStageSecretRef": {
		"name": "rook-csi-rbd-node",
		"namespace": "rook-ceph"
	},
	"volumeAttributes": {
		"clusterID": "rook-ceph",
		"imageFeatures": "layering",
		"imageFormat": "2",
		"imageName": "csi-vol-92837648-1431-11eb-8990-0242ac110005",
		"journalPool": "replicapool",
		"pool": "replicapool",
		"radosNamespace": "",
		"storage.kubernetes.io/csiProvisionerIdentity": "1603348800044-8081-rook-ceph.rbd.csi.ceph.com"
	},
	"volumeHandle": "0001-0009-rook-ceph-0000000000000002-92837648-1431-11eb-8990-0242ac110005"
}
```

In the above PV `replicapool` is the journal pool name so let's step into the toolbox
pod which helps us to connect to the ceph cluster

**_NOTE:_** If you are using a standalone ceph cluster you can execute these commands
from your ceph cluster

> Cephcsi internal design created 2 omap mapping
> * one is request ID(PV name) to unique ID mapping
>   * This helps to make cephcsi idempotent even if we get the same request
      we return the existing data.
> * One is uniqueID and image details mapping
>   * This helps cephcsi to extract the image details   when volumeID is passed in
      the request, cephcsi will decode the volumeID and get the omap
      details to extract image/volume name etc.

### RBD PV name and unique ID mapping

PV name is the request name, let's get the unique ID mapped to the request name

```bash=
[üé©Ô∏é]mrajanna@ceph $]kubectl exec -it rook-ceph-tools-6c984f579-qqh7n sh -nrook-ceph
sh-4.4# rados getomapval csi.volumes.default csi.volume.pvc-4a4c2fa3-086b-49fc-a1ef-fd8b0768f4b1 --pool=replicapool
value (36 bytes) :
00000000  39 32 38 33 37 36 34 38  2d 31 34 33 31 2d 31 31  |92837648-1431-11|
00000010  65 62 2d 38 39 39 30 2d  30 32 34 32 61 63 31 31  |eb-8990-0242ac11|
00000020  30 30 30 35                                       |0005|
00000024
```

`csi.volumes.default` is the object created by cephcsi to store the request
name to unique ID mapping

`92837648-1431-11eb-8990-0242ac110005` is the unique ID mapped to request name
`pvc-4a4c2fa3-086b-49fc-a1ef-fd8b0768f4b1`

### RBD unique ID and omap details mapping

Let's get the list of keys cephcsi stores with this unique ID

```bash=
sh-4.4# rados listomapkeys csi.volume.92837648-1431-11eb-8990-0242ac110005 --pool=replicapool
csi.imageid
csi.imagename
csi.volname
```

`csi.volume.92837648-1431-11eb-8990-0242ac110005` is the object cephcsi creates
to store individual image details and its always unique and last part of the
object is the unique ID that we got from Request Name mapping.

For RBD, cephcsi stores 3 keys in the unique object

1. csi.imageid is the key which holds the imageid which is required by cephcsi
   to deletion operation.
2. csi.imagename which holds the RBD image name.
3. csi.volname holds the request name.

Let's get all the values of these keys

**Note:** Am listing all the values stored in the
`csi.volume.92837648-1431-11eb-8990-0242ac110005` object

```bash=
sh-4.4# rados listomapvals csi.volume.92837648-1431-11eb-8990-0242ac110005 --pool=replicapool
csi.imageid
value (11 bytes) :
00000000  31 30 39 36 39 35 32 32  61 35 63                 |10969522a5c|
0000000b

csi.imagename
value (44 bytes) :
00000000  63 73 69 2d 76 6f 6c 2d  39 32 38 33 37 36 34 38  |csi-vol-92837648|
00000010  2d 31 34 33 31 2d 31 31  65 62 2d 38 39 39 30 2d  |-1431-11eb-8990-|
00000020  30 32 34 32 61 63 31 31  30 30 30 35              |0242ac110005|
0000002c

csi.volname
value (40 bytes) :
00000000  70 76 63 2d 34 61 34 63  32 66 61 33 2d 30 38 36  |pvc-4a4c2fa3-086|
00000010  62 2d 34 39 66 63 2d 61  31 65 66 2d 66 64 38 62  |b-49fc-a1ef-fd8b|
00000020  30 37 36 38 66 34 62 31                           |0768f4b1|
00000028
```

We have seen how to track the PV -> rados omap mapping for the RBD,let's see how
to track the CephFS rados omap data

## Track CephFS PVC omap data

To get the omap details we need to know the cephfs metadata pool in which
cephcsi stores the omap data. Let's get the filesystem name which is
stored in PV CSI spec

```bash=
[üé©Ô∏é]mrajanna@ceph $]kubectl get pv pvc-88919d42-ecdf-4737-a805-065eacdfd34f -o jsonpath='{.spec.csi}'
{
	"controllerExpandSecretRef": {
		"name": "rook-csi-cephfs-provisioner",
		"namespace": "rook-ceph"
	},
	"driver": "rook-ceph.cephfs.csi.ceph.com",
	"fsType": "ext4",
	"nodeStageSecretRef": {
		"name": "rook-csi-cephfs-node",
		"namespace": "rook-ceph"
	},
	"volumeAttributes": {
		"clusterID": "rook-ceph",
		"fsName": "myfs",
		"pool": "myfs-data0",
		"storage.kubernetes.io/csiProvisionerIdentity": "1603348799452-8081-rook-ceph.cephfs.csi.ceph.com",
		"subvolumeName": "csi-vol-bff8a308-1431-11eb-b0fd-0242ac110006"
	},
	"volumeHandle": "0001-0009-rook-ceph-0000000000000001-bff8a308-1431-11eb-b0fd-0242ac110006"
}
```

In the above PV `myfs` is the CephFS filesystem name so let's step into the
toolbox pod which helps us to connect to ceph cluster

**Note:** If you are using standalone ceph cluster you can execute these commands
from your ceph cluster

```text
To know the metadata pool for the filesystem, run
sh-4.4# ceph fs ls
name: myfs, metadata pool: myfs-metadata, data pools: [myfs-data0 ]
```

### CephFS PV name and unique ID mapping

PV name is the request name, let's get the unique ID mapped to the request name

```bash=
[üé©Ô∏é]mrajanna@ceph $]kubectl exec -it rook-ceph-tools-6c984f579-qqh7n sh -nrook-ceph
sh-4.4# rados getomapval csi.volumes.default csi.volume.pvc-88919d42-ecdf-4737-a805-065eacdfd34f --pool=myfs-metadata --namespace=csi
value (36 bytes) :
00000000  62 66 66 38 61 33 30 38  2d 31 34 33 31 2d 31 31  |bff8a308-1431-11|
00000010  65 62 2d 62 30 66 64 2d  30 32 34 32 61 63 31 31  |eb-b0fd-0242ac11|
00000020  30 30 30 36                                       |0006|
```

`csi.volumes.default` is the object created by cephcsi to store the request
name to unique ID mapping, For CephFS cephcsi uses `csi` namespace by
default,for RBD its `default` rados namespace.

`bff8a308-1431-11eb-b0fd-0242ac110006` is the unique ID mapped to request name
`pvc-88919d42-ecdf-4737-a805-065eacdfd34f`

### CephFS unique ID and omap details mapping

Let's get the list of keys cephcsi stores with this unique ID

```bash=
sh-4.4# rados listomapkeys csi.volume.bff8a308-1431-11eb-b0fd-0242ac110006 --pool=myfs-metadata --namespace=csi
csi.imagename
csi.volname
```

`csi.volume.bff8a308-1431-11eb-b0fd-0242ac110006` is the object cephcsi creates
to store indivisual image details and its always unique and last part of the
object is the unique ID that we got from Request Name mapping.

For CephFS, cephcsi stores 2 keys in the unique object

1. csi.imagename which holds the CephFS subvolume name.
2. csi.volname holds the request name.

Let's get all the values of these keys

Note:- Am listing all the values stored in the
`csi.volume.bff8a308-1431-11eb-b0fd-0242ac110006` object

```bash=
sh-4.4# rados listomapvals csi.volume.bff8a308-1431-11eb-b0fd-0242ac110006 --pool=myfs-metadata --namespace=csi
csi.imagename
value (44 bytes) :
00000000  63 73 69 2d 76 6f 6c 2d  62 66 66 38 61 33 30 38  |csi-vol-bff8a308|
00000010  2d 31 34 33 31 2d 31 31  65 62 2d 62 30 66 64 2d  |-1431-11eb-b0fd-|
00000020  30 32 34 32 61 63 31 31  30 30 30 36              |0242ac110006|
0000002c

csi.volname
value (40 bytes) :
00000000  70 76 63 2d 38 38 39 31  39 64 34 32 2d 65 63 64  |pvc-88919d42-ecd|
00000010  66 2d 34 37 33 37 2d 61  38 30 35 2d 30 36 35 65  |f-4737-a805-065e|
00000020  61 63 64 66 64 33 34 66                           |acdfd34f|
00000028
```]]></content><author><name></name></author><summary type="html"><![CDATA[This blog will help you to understand how to track the internal rados omap data stored by cephcsi for cephfs and rbd pvc.]]></summary></entry><entry><title type="html">Install Rook in minikube</title><link href="https://mrajanna.com/setup-minikube-rook/" rel="alternate" type="text/html" title="Install Rook in minikube" /><published>2020-10-11T00:00:00+02:00</published><updated>2020-10-11T00:00:00+02:00</updated><id>https://mrajanna.com/setup-minikube-rook</id><content type="html" xml:base="https://mrajanna.com/setup-minikube-rook/"><![CDATA[This blog will help you out to install and setup rook in a minikube vm, before
you continue make sure you have installed minikube on your local system

## Install minikube

```bash=
[üé©Ô∏é]mrajanna@localhost $]curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64
[üé©Ô∏é]mrajanna@localhost $]sudo install minikube-linux-amd64 /usr/local/bin/minikube
```

## create kubernetes cluster using minikube

```bash=
[üé©Ô∏é]mrajanna@localhost $]minikube start --force --memory="4096" --cpus="2" -b kubeadm --kubernetes-version="v1.19.2" --driver="kvm2" --feature-gates="BlockVolume=true,CSIBlockVolume=true,VolumeSnapshotDataSource=true,ExpandCSIVolumes=true"
```

I will be using `kvm2` in this blog as a vmdriver to  create minikube vm, rook
expects us to have a raw device on the nodes where we are creating a ceph
cluster. using kvm2 we can attach devices to the minikube vm.You can also
select different vm drivers when installing the minikube.

### Create a folder for rook to store the ceph information

```bash=
[üé©Ô∏é]mrajanna@localhost $]minikube ssh "sudo mkdir -p /mnt/vda1/var/lib/rook;sudo ln -s /mnt/vda1/var/lib/rook /var/lib/rook"
```

### Add a disk to minikube vm

```bash=
[üé©Ô∏é]mrajanna@localhost $]sudo -S qemu-img create -f raw /var/lib/libvirt/images/minikube-box-vm-disk-50G 50G
[üé©Ô∏é]mrajanna@localhost $]virsh -c qemu:///system attach-disk minikube --source /var/lib/libvirt/images/minikube-box-vm-disk-50G --target vdb --cache none
[üé©Ô∏é]mrajanna@localhost $]virsh -c qemu:///system reboot --domain minikube
```

Assuming you have already installed libvirt virsh etc. Am attaching a device
called `vdb` to the minikube vm to create ceph cluster.

Note:

```note
You can do `minikube ssh` and step inside the minikube vm and check `/dev/vdb` is created.
sometimes the disk wont show up immidiately for that you can need to  start
the minikube again.
```

```bash=
[üé©Ô∏é]mrajanna@localhost $]minikube ssh
$ ls /dev/vdb
$ exit
[üé©Ô∏é]mrajanna@localhost $]minikube start --force --memory="4096" --cpus="2" -b kubeadm --kubernetes-version="v1.19.2" --driver="kvm2" --feature-gates="BlockVolume=true,CSIBlockVolume=true,VolumeSnapshotDataSource=true,ExpandCSIVolumes=true"
```

to verify the kubernetes cluster  is created you can run below command

```console
[üé©Ô∏é]mrajanna@localhost $]minikube kubectl -- cluster-info
```

### Install Rook

As the kubernetes cluster is installed we can start installing the rook now,
for that we need to first download the  rook github project and check out the
release branch.

```bash=
[üé©Ô∏é]mrajanna@localhost $]git clone git@github.com:rook/rook.git
[üé©Ô∏é]mrajanna@localhost $]git checkout v1.4.5
```

All the kubernetes templates which are required for the rook installation are
localted at `cluster/examples/kubernetes/ceph`

```bash=
[üé©Ô∏é]mrajanna@localhost $]cd rook/cluster/examples/kubernetes/ceph
```

To have a complete ceph cluster, we need to install below yaml files

```text
common.yaml         ----> CRD's and RBAC's required for operator
operator.yaml       ----> Operator deployment
cluster-test.yaml   ----> The ceph cluster CRD
pool-test.yaml      ----> block pool CRD
filesystem.yaml     ----> ceph filesystem CRD
toolbox.yaml        ----> toolbox deployment to execute ceph commands
```

Files ending with *_test.yaml* should be used only for testing not for
production.

Lets create all the kubernetes templates to create ceph cluster

```bash=
kubectl create -f common.yaml
kubectl create -f operator.yaml
kubectl create -f cluster-test.yaml
kubectl create -f pool-test.yaml
kubectl create -f filesystem.yaml
kubectl create -f toolbox.yaml
```

Lets wait for few minutes and Verify all the pods are running

```bash=
[üé©Ô∏é]mrajanna@localhost $]kubectl get po -nrook-ceph
NAME                                            READY   STATUS      RESTARTS   AGE
csi-cephfsplugin-7llbt                          3/3     Running     0          22h
csi-cephfsplugin-provisioner-5c65b94c8d-tljqd   0/6     Pending     0          22h
csi-cephfsplugin-provisioner-5c65b94c8d-x4kgq   6/6     Running     8          22h
csi-rbdplugin-b6jgs                             3/3     Running     0          22h
csi-rbdplugin-provisioner-569c75558-7twxh       0/6     Pending     0          22h
csi-rbdplugin-provisioner-569c75558-kx69l       6/6     Running     8          22h
rook-ceph-mds-myfs-a-6dd4596558-lt94j           1/1     Running     0          22h
rook-ceph-mds-myfs-b-77986b766d-rlc2v           1/1     Running     0          22h
rook-ceph-mgr-a-75f9c8bd4b-gwvvx                1/1     Running     2          22h
rook-ceph-mon-a-5f85c959bf-gmssg                1/1     Running     0          22h
rook-ceph-operator-6db6f67cd4-7wmkq             1/1     Running     0          22h
rook-ceph-osd-0-77b585d64-5pfcq                 1/1     Running     0          22h
rook-ceph-osd-prepare-minikube-hqqj7            0/1     Completed   0          15h
rook-ceph-tools-6c984f579-m6ccc                 1/1     Running     0          22h
rook-discover-hjj6c                             1/1     Running     0          22h
```

Check ceph filesystem  and block pool is create

```bash=
[üé©Ô∏é]mrajanna@localhost $]kubectl -n rook-ceph get cephfilesystems myfs
NAME   ACTIVEMDS   AGE
myfs   1           22h
[üé©Ô∏é]mrajanna@localhost $]kubectl -n rook-ceph get  cephblockpools replicapool
NAME          AGE
replicapool   22h
```

Let exec into the ceph toolbox pod and verify `ceph status`, pools and
filesystem  created in ceph.

```bash=
[üé©Ô∏é]mrajanna@localhost $]kubectl exec -it rook-ceph-tools-6c984f579-m6ccc sh -nrook-ceph
sh-4.4# ceph -s
  cluster:
    id:     f50de5f0-5d0a-4621-a6fb-04192efdcc79
    health: HEALTH_OK

  services:
    mon: 1 daemons, quorum a (age 22h)
    mgr: a(active, since 112m)
    mds: myfs:1 {0=myfs-b=up:active} 1 up:standby-replay
    osd: 1 osds: 1 up (since 113m), 1 in (since 22h)

  task status:
    scrub status:
        mds.myfs-a: idle
        mds.myfs-b: idle

  data:
    pools:   4 pools, 97 pgs
    objects: 35 objects, 3.2 MiB
    usage:   1.0 GiB used, 49 GiB / 50 GiB avail
    pgs:     97 active+clean

  io:
    client:   1.2 KiB/s rd, 2 op/s rd, 0 op/s wr

sh-4.4# ceph osd lspools
1 device_health_metrics
2 replicapool
3 myfs-metadata
4 myfs-data0
sh-4.4# ceph fs ls
name: myfs, metadata pool: myfs-metadata, data pools: [myfs-data0 ]
sh-4.4#
```

Now we have create ceph cluster using rook in minikube. The commands we have
exectuted is available as a
[shell-script](https://gist.github.com/Madhu-1/2f5db960884671942540f06c599e50c2)]]></content><author><name></name></author><summary type="html"><![CDATA[This blog will help you out to install and setup rook in a minikube vm, before you continue make sure you have installed minikube on your local system]]></summary></entry></feed>